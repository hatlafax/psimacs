:PROPERTIES:
:ID:       b1bbf8c7-24e4-4d60-a1ac-d7e511aade32
:END:

#+TITLE: Point Cloud Technology in the context of VisualEvents
#+OPTIONS: toc:nil author:nil
#+OPTIONS: d:nil prop:nil
#+OPTIONS: tex:t
#+OPTIONS: tags:nil
#+OPTIONS: ^:{}

#+EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: apa7
#+LaTeX_CLASS_OPTIONS: [a4paper, biblatex]

#+LaTeX_HEADER: \author{Johannes Brunen}
#+LaTeX_HEADER: \authorsaffiliations{DataSolid GmbH, Mönchengladbach}
#+LaTeX_HEADER: \leftheader{Brunen}
#+LaTeX_HEADER: \shorttitle{point clouds}

#+LaTeX_HEADER: \usepackage{breakcites}
# #+LaTeX_HEADER: \usepackage{apacite}
# #+LaTeX_HEADER: \usepackage{natbib}

# #+latex_header: \usepackage{subcaption}
#+LaTeX_HEADER: \usepackage{subfig}
#+LaTeX_HEADER: \usepackage{paralist}

#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\scriptsize}

#+LaTeX_HEADER: \usepackage{xurl}
#+LaTeX_HEADER: \setcounter{biburllcpenalty}{7000}
#+LaTeX_HEADER: \setcounter{biburlucpenalty}{8000}

#+LaTeX_HEADER: \let\itemize\compactitem
#+LaTeX_HEADER: \let\description\compactdesc
#+LaTeX_HEADER: \let\enumerate\compactenum

#+LaTeX_HEADER: \abstract{This articel investigates the state of the art technology of point cloud processing.
#+LaTeX_HEADER: Data formats, tools and programming libraries are examined. The goal is to make use of these
#+LaTeX_HEADER: technologies for the \emph{VisualEvents} application
#+LaTeX_HEADER: stack of the \emph{VisualEvents GmbH \& Co. KG} company.}
#+LaTeX_HEADER: \keywords{VisualEvents, point clouds}
#+LateX_HEADER: \hypersetup{colorlinks=true, urlcolor = blue, citecolor = black, linkcolor = black}

#+csl-style: apa-numeric-superscript-brackets.csl
#+csl-locale: en-US

* Agenda                                                           :noexport:
** Vorstellung der bisherigen Tätigkeiten
** Ziele, Erwartungen User Stories
** AWS, RacerTek und Machinelles Lernen

* Introduction                                                      :publish:
VisualEvents[[cite:&VisualEvents]] is a Web application from DataSolid GmbH[[cite:&DataSolid]] that is concerned with the the utilization and rendering of
inside spaces. These spaces are typically provided as 3D point clouds from 3D laser scanner scans or photogrammetry
software[[cite:&enwiki:1086318007]]. The amount of point cloud data created by these techniques can be immense. For
instance the [[https://leica-geosystems.com/products/laser-scanners/scanners/leica-rtc360?_ga=2.202064657.1429646317.1655791176-282793318.1655791176][Leica RTC360 3D Laser Scanner]] acquires up to 2 million points per second[[cite:&leicartc360]]. Therefore
intelligent and efficient handling and processsing of point cloud data is mandatory.

This document tries to provide an overview about the relevant technology in the domain of point cloud acquisition and
processing.

* Questions                                                         :publish:
- What is our goal?
- What are the real problems that we have?
- What is the duty of any /pipeline/ or /workflow/ that we target?
- What are the objects that we want to recognize?
- What level of detail is important for object recognition?
- Why do we want to recognize these objects?
- Why is it important to use machine learning for our purpose?
- What are the hardware resources that are needed for such an endeavor?
- Where should the training data come from?
- Where should the testing and validation data come from?
- What are the quality constraints for these trainings data?
- What are the quality constraints for the input data?
- Are the input data homogenous?
- What about coloring and texturing?
- What about facetting and mesh generation?
- What are the constraints for rendering?

* Point Cloud Data Aquisition                                       :publish:
** Point Cloud Artefacts
*** Misadjusted Density

Point clouds exhibit locally variable densities based on surface orientation,
nature and distance from the capturing device. Occlusions from surface irregularities
and adjacent objects also produce regions with missing data. Variations in density
can be attenuated by subsampling techniques but not fully eliminated.

*** Clutter

A scene can contain small objects represented by very few points,
moving objects and multiple objects in proximity which for an
application are considered /noise/. These are often making feature
detection, structuration and automatic recognition difficult.

*** Occlusion

A scene can contain objects of significant size that occlude objects
behind them. This produces incomplete and
disjointed descriptions of the background surfaces thus large missing
areas.

*** Random Errors

This is mainly due to absorption linked to the operation frequency,
scattering, and taking into account the properties of the observed
object and the wavelength of the incident energy. These influence the
sensor's choice considering the application, firstly between active and
passive sensors. As passive sensors rely solely on the light emittance of
the measured object, they are more influenced by atmospheric
conditions and errors.

*** Systematic Errors

The sensor will measure a property in the scene, and to be highly
representative, must be sensitive to only the value measured, without
influencing the backscattered signal. However, errors such as zero
input, sensitivity error will create additional noise, but these can be
calibrated.

*** Surface Properties

The physical texture of common surfaces can range from smooth
(steel, marble) to very irregular (grass, crushed stone). Because a
given scene can contain a wide range of surface roughness’s, no priors
about noise levels can be reliably used.

*** Misalignement

Assembling point data in one reference system is a task that is
primarily dependent on the acquisition platform.
Furthermore, registration can introduce local noise due to imperfect
correspondence between point clouds.

** Overview on Scanning and Reconstruction Methods

The publication of V. V. Lehtola et al.[[cite:&Lehtola2021]] provides a survey and wealth of information
about state of the art scanning and reconstruction methods.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 180px
#+caption: Capturing systems and basic data for the creation of 3D point clouds.
#+name:   fig:acquisitionTechniques1
[[./acquisitionTechniques1.PNG]]

*** What scanning and reconstruction methods are needed for /indoor spaces/?

Different single sensor methods exists:

- (a) Depth cameras RGB-D
- (b) Photogrammetry RGB
- (c) Rolling 2D scanner
- (d) Lidar

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+caption: Scanning platforms.
#+name:   fig:scanningPlatforms
[[./ScannerTec.png]]

Multi-sensor systems are common in mobile mapping of indoor spaces.

- human-carriable systems.
- mobile platforms.

Micro aerial vehicles (MAV) offer maneuverability and flexibility in mapping
indoor spaces.

*** What is the purpose of 3D reconstruction?
- Schematic models for engineering purposes, e.g. /BIM/.
- Visual models that are intended for a broader audience.

3D indoor reconstruction is the process of generating a mesh
model which is exportable to one of the standard formats.
In other words, the point clouds are replaced by a mesh that consists of
continuous geometrical shapes such as planes and boundary representations.
During a reconstruction process, a successful composition of walls is the
most important factor because it defines the main layout of the interiors.
The reconstruction process here includes the data segmentation step, where
the point cloud is divided into rooms and subspaces.

*** What are specific Indoor Reconstruction problems?
- Positioning problem.

  The position of the scanning system, when developed as a function of time, becomes the
  traversed path of motion, i.e. the trajectory.

  - Dead Reckoning problem.
  - The key in indoor scanning is the robustness of the positioning method.

  There, the basic idea is to track the position and heading (i.e. pose)
  of the sensor system as a function of time in 3D relative coordinates. The pose
  updates are done using the overlaps in optical data, that is for example keeping
  record on déjà-vu’s, or technically, features that have been seen before. In robotics,
  this is known as the simultaneous localization and mapping (SLAM).

  The scanning trajectory is of critical importance in each of the steps towards the final 3D
  model, i.e. an application-suitable digital twin of the indoor space. The concept of
  trajectory, i.e. the path that the scanning system has traveled, is at the very core of
  mobile mapping[[cite:&Lehtola2021]].

  - Relative Positioning, e.g. SLAM.
  - Global Positioning, e.g. GNSS.

  GNSS signals are not available indoors, relative positioning methods (SLAM) must be employed.

- Lighting and exposure

  Indoor spaces might be dark or over-illuminated.

- Surface texturing

  Can be colorful or lacking texture.

- Reflections, mirrors

  Reflections of optical rays may occur
  from transparent surfaces, e.g. glass, or shiny surfaces, e.g. metal. With digital
  images, light sources are probably the most common cause for reflections from
  surfaces.

- Restricted sensory trajectory

  The sensor trajectory is more restricted and difficult measuring geometries
  that may lead to registration problems are encountered for example in narrow
  doorways.

- Highly convoluted spaces
- Walls

  Difficulty in distinguishing the points captured from the two different
  sides of a thin wall.

- Openings

  Distinguishing between an opening caused by missing data and an opening
  caused by an existing window.

- Occlusions

  This likely, requires an online interface from which the operator can see what
  part of the area needs further scanning.

  - Dynamic occlusion, e.g. people.
  - Static occlusion, e.g. objects.

- Multi-Scale Problem

  The magnitude of sizes varies from the order of one centimeter to dozens or even
  hundreds of meters. In other words, the characteristic length scale of interior spaces
  spans four orders of magnitude.

  The multi-scale problem sets apparently conflicting criteria to the design of the
  indoor mapping system. On one hand, the sampling resolution should be large to be
  able to account for the smallest details, but on the other hand it should be sparse to make
  covering large spaces computationally tractable.

  An ideal system designed for three-dimensional (3D) indoor reconstruction has a sampling
  rate that can account for the smallest details, but is able to do efficient data
  distillation so that even the largest interior spaces may be covered. Usually for
  applications, it is important that the level of detail stays the same regardless of
  the size of the building. It determines the properties that the measurement system
  should fulfill.

- Optical sensor capabilities
- Field of view

  Scanning techniques must account for not being able to see the surrounding
  indoor space in one snapshot, as sensors typically have a field of view that
  does not cover 360 degree rotation around two directions.

- Outliers

  Outliers in indoor 3D data may be considerably harder to eliminate than the ones
  present in 3D object data because objects have a simple (convex hull) topology
  while indoor spaces usually do not. In other words, while outlier points inside a 3D
  object are harmless, they are a problem inside a room.

- Explicit indorr surfaces

  When scanning separate objects, e.g. by moving a camera around them,
  it is typically assumed that the surface of that object does not contain any holes, i.e.
  that the surface is implicit. This assumption greatly facilitates the reconstruction,
  because then a coherent surface without holes is always recovered. However, this
  assumption must be relaxed for indoor spaces, because for example windows (or
  arbitrary decorations) form holes on the walls (or other surfaces). This, that all
  indoor surfaces are explicit, makes the reconstruction process significantly harder
  than what it is for single objects.


| Conditions   | RGB     | RGB-D   | TLS     | RGB-TLS  |
|--------------+---------+---------+---------+----------|
| Nominal      | Y       | Y       | Y       | Y        |
| Textures     | N       | Y       | Y       | Y        |
| Dark         | N       | N       | Y       | Y        |
| Direct light | N       | N       | Y       | Y        |
| Sunlight     | N       | N       | Y       | Y        |
| Advantage    | Tex/geo | Tex/geo | Geo     | Tex/geo  |
| Range        |         | 6–10m   | 30–100m | 30–100+m |
|--------------+---------+---------+---------+----------|

*** History
The creation of indoor 3D models from scanned data was mainly a curiosity
before 2010s, and was done without modern mobile mapping methods[[cite:&Lehtola2021]]
and relied on 3D point clouds obtained from /terrestrial laser scanning (TLS)/.
or on classical photogrammetry.

- TLS scanning expensive high-cost equipment.
- Photorgrammetry has problems with
  - camera calibration,
  - lighting
  - texturing
  - complex indoor environments

*** Space Subdivision and Room Segmentation

/Space subdivision/ is referred to the problem of dividing the space into /semantic subspaces/.
Another term used in the literature for space subdivision is /room segmentation/.
However, there are slight differences between the concepts of a /room/
and a /subspace/. A /room/ is separated from other rooms by permanent structures
such as walls, floors and ceilings and there should be an opening (e.g. door) to
connect two rooms. A /subspace/ can represent a room or part of a room, for example
a meeting area which is separated from the rest of that room by temporary partitions.

Considerations with respect to space subdivision algorithms:

- The space subdivision can be done in 2D, 2.5D or 3D.
- The space subdivision can be done with or without the Manhattan-World assumptions.
- The trajectory of the acquisition device, in case of mobile laser scanners, can be a valuable data source for the space subdivision.

Lehtola[[cite:&Lehtola2021]] provides a survey about the most common space subdivision methods.

*** Detection and Reconstruction of Openings

It turns out that in most of indoor environments the structural surfaces are actually occluded,
and as a consequence there are holes, for example, on the walls, which necessarily do not
represent openings. The challenge then is to discriminate between these holes caused by occlusion
and the holes which represent windows or doorway openings.

*** Photogrammetry

- Pros
  - Size doesn’t matter

    Depending on the camera you use, you can capture objects as small as a toy and as big as a
    statue. You can also take photos with a drone to capture really big things. The only thing
    you should be careful with is the depth of field when capturing really small things. You
    basically want everything in focus, which is impossible with macro photography.

  - The geometry quality can be incredible

    When using a camera with a sharp lens and many megapixels, Photogrammetry can achieve
    incredibly detailed results that can match professional 3D scanners that cost upwards
    of $15,000. But this really depends on the visual details of the object you’re scanning.

  - The texture quality is always incredible

    What always sets photogrammetry apart from even the most expensive industrial 3D scanners
    is texture quality. This can make up for less detailed geometry and can also be used to
    generate special texture maps like normal maps and displacement maps that can enhance
    the realism of 3D models without adding extra geometry.

- Cons

  - It requires experience

    To make photogrammetry work, you’ll first have to understand how it works and how you can
    take the photos in a way that’s optimal for a computer to understand.
    The problem is you have no zero feedback while taking pictures and not all problems that
    arise in processing can easily be fixed.

    But the experience is not just in shooting the photos. To get the best quality out of Photogrammetry
    software, you’ll also have to learn what impact certain settings have on the quality/processing time
    balance.

  - It’s slow

    While using a multi-camera Photogrammetry rig is the fastest way to 3D capture objects and people,
    doing it with a single camera requires time and patience. Shooting from a tripod is a must and to
    get the best shots you’ll sometimes use longer exposure times. And even for small to medium objects,
    you will have to shoot a minimum of 40-50 photos from various angles to get good results.

    Processing the photos can also take a long time. Greatly depending on the amount and size of the photos
    and the specs of your computer, it’s not unusual to have to wait hours before getting results. Some programs
    require so many resources that you cannot do anything else on the computer in the meantime. It’s good to know
    that many modern Photogrammetry programs are optimized for GPU-acceleration. In most cases, one or multiple
    graphics cards can be used to speed up the algorithms significantly. And in most cases, those from Nvidia are
    the best (or only) option.

    The problem of speeds wil become less of a problem in the future because algorithms are getting more
    efficient and computing hardware is becoming faster and more affordable. Both as it stands now: patience is a virtue.

  - Motion is Killing

    While most photogrammetry algorithms can handle moving objects in the far background, they can really go haywire
    when larger objects or people move even a little bit. But keep in mind that in difficult cases, you can always
    mask out distracting elements before processing. But again, this takes time.

  - It’s not to scale by default

    Most photogrammetry programs try to scale the model based on camera and lens information but this is always an
    estimate. So it’s always a good idea to take some reference measurements of the objects your’e capturing or
    put something in the scene of known size to scale the model later.

*** LiDAR vs. Photogrammetry

LiDAR is a technology that uses lasers in order to measure distances from the sensor on the LiDAR device to
objects in the environment. Photogrammetry on the other hand, is the art of making a three dimensional scale
model from a set of photographs taken from different angles.

LiDAR advantages:
- High accuracy.
- High-resolution.
- Creates highly accurate 3D maps.
- Can quickly capture and process data.
- Offers clean and sharp point clouds that are easy to work with.
- LiDAR typically requires only 20-30% image overlap.
- Raw data only requires several minutes of calibration (between 5 and 30 minutes) in order to produce the final results.

LiDAR disadvantages:
- Expensive technology.
- Requires a lot of hardware.
- Produces very dense point clouds.
- Problems stemming from the distortions in the laser beam.
- Typically no realistic images, colors, textures or features.
- Expensive oost-processing software.
- Bulky.
- Sensitivity to environmental conditions such as fog and dust.
- Problematic on reflective surfaces.

Photogrammetry advantages:
- Has lower error rates and can produce more accurate measurements and designs.
- Photorealistic interpretation of the original model due to color and texture quality.
- Lower entry barrier.
- Low cost.

Photogrammetry disadvantages:
- Inaccurate on close-up photos (< 0.5m).
- Expensive when aiming for highest quality.
- Significantly slower.
- Requires a lot of photos.
- Photogrammetry requires 60-90% image overlap.
- Time spent in the field can be multiplied by 5-10 fold for the data processing side.

The biggest difference in the output from both techniques is color. Point clouds made with
photogrammetry have a quality RGB value for each point.

*** Photogrammetry Workflows

Following a list of workflows for photogrammetry that is publicly available.

- [[https://unity3d.com/files/solutions/photogrammetry/Unity-Photogrammetry-Workflow_2017-07_v2.pdf][Unity Photogrammetry Workflow]] and the corresponding [[https://www.youtube.com/watch?v=Ny9ZXt_2v2Y][Siggraph 2017]] youtube video.
- [[https://rd.nytimes.com/projects/capturing-images-for-photogrammetry][Capturing Images for Photogrammetry]] from the R&D of the New York Times.
- [[https://www.youtube.com/watch?v=SzobKDdghGo][5 Common Mistakes when Photographing for Photogrammetry]]
- [[https://www.youtube.com/watch?v=JLdxBtECGuc][Photoscanning - Camera Settings]]
- [[https://www.youtube.com/watch?v=ZN8-tzqBLTs][What Camera is Best for Photoscanning?]]
- [[https://blendermarket.com/products/photogrammetry-course][Photogrammetry Course: Photoreal 3d With Blender And Reality Capture]]
- [[https://developer.valvesoftware.com/wiki/SteamVR/Environments/Advanced_Indoors_Photogrammetry][SteamVR/Environments/Advanced Indoors Photogrammetry]]

** Photogrammetry Software

The [[https://www.sculpteo.com/en/3d-learning-hub/3d-printing-software/photogrammetry-software/][article]] /Best photogrammetry software in 2022/ gives a short overview of the
free and commercial software for photogrammetry based registration.
Another great overviews are provided by the [[https://80.lv/articles/80-level-ratings-great-photogrammetry-software-and-youtube-channels/][article]] /Great Photogrammetry Software and YouTube Channels/
and by the [[https://www.selfcad.com/blog/5-best-photogrammetry-software][blog]] /5 Best Photogrammetry Software of 2022/.
And at last the [[https://formlabs.com/blog/photogrammetry-guide-and-software-comparison/][blog]] /Photogrammetry: Step-by-Step Guide and Software Comparison/
gives additional information and a more substantial comparsion.

The following is a list of the more interesting products:

- [[https://www.agisoft.com/][Agisoft Metashape]] commercial with free narrowed version.

  Metashape allows the [[https://agisoft.freshdesk.com/support/solutions/articles/31000151726-how-to-import-external-point-cloud-e-g-lidar-to-metashape][import]] of LiDAR or photogrammetry point clouds obtained from external
  sources into Metashape project to be merged with the dense cloud generated in Metashape or
  even to substitute it[[cite:&LidarAndPhotogrammetryAgisoft]].

- [[https://alicevision.org/#][AliceVision Meshroom]] free and used for experiments. Processing time is roughly 30sec per input image.
- [[https://colmap.github.io/index.html][Colmap]] general-purpose pipeline with a graphical and command-line interface.
- [[https://www.capturingreality.com/Home][Reality Capture]] commmercial, cloud based.
- [[https://www.pix4d.com/product/pix4dmapper-photogrammetry-software][PIX4Dmapper]] commercial for drone mapping.

** Indoor Photogrammetry

Building interiors, small spaces and flat surfaces: indoor mapping is a challenge for photogrammetry.
Various image shooting methods has been tested and compared[[cite:&indoorPhotogrammetryPix4d]].

The following [[https://www.youtube.com/watch?v=8HuOvf4rKaw][Webinar]] of Pix4D gives a rought overview about the state the art of 2014.

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Using this method, the sharp intersection angles do not generate good matches.]{
  \includegraphics[width=.3\linewidth]{./acquisition-orientation-2.jpg}
}
\qquad
\subfloat[][The disantantage of this method is that matches were only found on the ground.]{
  \includegraphics[width=.3\linewidth]{./acquisition-orientation-3.jpg}
}
\qquad
\subfloat[][Similar, the matches were concentrated on the ground.]{
  \includegraphics[width=.3\linewidth]{./acquisition-orientation-4.jpg}
}
\qquad
\subfloat[][This method provides the most well-distributed matches.]{
  \includegraphics[width=.3\linewidth]{./acquisition-orientation-1.jpg}
}
\caption{Aquisition methods.}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Using this method, the sharp intersection angles do not generate good matches.
#+name:   fig:acquisition-orientation-2
[[./acquisition-orientation-2.jpg]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: The disantantage of this method is that matches were only found on the ground.
#+name:   fig:acquisition-orientation-3
[[./acquisition-orientation-3.jpg]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Similar, the matches were concentrated on the ground.
#+name:   fig:acquisition-orientation-4
[[./acquisition-orientation-4.jpg]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: This method provides the most well-distributed matches.
#+name:   fig:acquisition-orientation-1
[[./acquisition-orientation-1.jpg]]
#+end_export


#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 120px
#+caption: Visualization of the winning testing method, including corrections and shooting directions.
#+name:   fig:InddorShooting
[[./reconstruction-3.jpg]]

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 120px
#+caption: Indoor room aquisition.
#+name:   fig:InddorShooting
[[./IndoorShooting1.png]]

*** Photogrammetry / Room Scanning test

The following youtube video [[https://www.youtube.com/watch?v=klZmGoq3fIo][Photogrammetry / Room Scanning test]] shows a room photogrammetry scan
of 728 photos taken with a Sony A7 I 24mp equipped with a fullformat 24mm Kitlens using a tripod.

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven1.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven2.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven3.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven4.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven5.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven6.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven7.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Sven8.PNG}
}
\caption{Screenshots from the Room Scanning Test video.}
\label{fig:Room Scanning Test}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 1 from the Room Scanning Test video.
#+name:   fig:Sven1
[[./Sven1.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 2 from the Room Scanning Test video.
#+name:   fig:Sven2
[[./Sven2.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 3 from the Room Scanning Test video.
#+name:   fig:Sven3
[[./Sven3.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 4 from the Room Scanning Test video.
#+name:   fig:Sven4
[[./Sven4.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 5 from the Room Scanning Test video.
#+name:   fig:Sven5
[[./Sven5.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 6 from the Room Scanning Test video.
#+name:   fig:Sven6
[[./Sven6.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 7 from the Room Scanning Test video.
#+name:   fig:Sven7
[[./Sven7.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Screenshot 8 from the Room Scanning Test video.
#+name:   fig:Sven8
[[./Sven8.PNG]]
#+end_export

*** SteamVR/Environments/Advanced Indoors Photogrammetry

The [[https://developer.valvesoftware.com/wiki/SteamVR/Environments/Advanced_Indoors_Photogrammetry][article]][[cite:&SteamVRAdvIndoor]] provides an overview of the work to be done to
do indoor reconstruction based on photogrammetry.

"Interior architectural scenes can be some of the hardest to scan well. Featureless, blank
painted surfaces, reflections, specularity, the exacting perfection needed to make clean,
manufactured surfaces not look distorted and misshapen - even the best source photography
will fail to capture everything."[[cite:&SteamVRAdvIndoor]]

The article is well written and shows the process of 3D indoor reconstruction and all
the effort that is needed to get a finally good result. It is well worth to study the
images embedded in the article.

**** Take aways from this article:
- Your need a quality DLSR/DLSM camera setup with a prime lens and a tripod.
- Lens image stabilization and auto focus worsen the results and should not be used.
- Wide angle lenses are recommended.
- In case of using zoom lenses taping of of the zoom ring and focus ring is recommended in order to prevent any accidental shifts.
- Set to manual focus at an approximate hyperfocal distance[[cite:&enwiki:1079288045]] of 1m for maximum depth of field.
- Keep focus fixed across shots.
- Image numbers go to the thousends, better to take too many photos than too few.
- Substantial experience in photography is necessary.
- Required feature with respect to the photogrammetry software: the ability to re-import cleaned up meshes into the for reprojecting new textures.
- Impossible to scan: Glass, glossy surfaces and featureless walls.
- Control points might be necessary.
- Resulting mesh can easily go to 400 millions triangles. Decimation required.
  Three million triangles for an unlit mesh is a maximum for VR.
- Reconstructed photogrammetry scenes by far do never have production quality.
- Walls, floor, ceiling and windows needs reconstruction.
- Errors needs to be corrected by hand.
- Cleanup on a large, interior scene can be extremely time-consuming.
- (Re)generation of UVs is always necessary after changing the reconstruction.

**** Requirements and Constraints:
- No automatic process is known for photogrammetry.
- Time consuming data aquisition.
- Expert photography knowledge required.
- Registration is very time consuming (days of processing time).
- Complex technology stack necessary for postprocessing.
- Technology stack needs export knowledge to be of use.
- Postprocessing is also very time consuming even for experts.
- Foreign know-how is required.

**** Overall quintessence:

Photogrammetry is *not* the correct technique for us to go. Even with expertise
and high quality camera gear the reconstructed results are not even near
production quality and always do need costly postprocessing by domain experts.

** LiDAR

/LiDAR/ is an acronym for light detection and ranging, which is also known as 3D laser scanning.
LiDAR was first used in the 1960 and is wielded for measuring distances based on laser light.
It works by illuminating a target object or space with a laser light and recording the time
it takes the laser light to return to the sensor to measure distances with high accuracy.

3D laser scanning is already in use in a variety of industries, with applications ranging
from mobile to terrestrial and airborne data collection projects. Because laser scanning
measures distances accurately, it is highly effective for making digital 3D recreations
of space, objects, and landscapes. In this aspect it can compete with photogrammetry as a
tool for digitizing reality. LiDAR methods produce a geo-referenced and non-colored 3D
point cloud. This dense point cloud is highly detailed and often includes small objects,
such as cables or wires, that photogrammetry may not recognize as easily. However,
LiDAR can be expensive to perform, and it is often necessary to hire a specialized
laser scanning company to gather data.

LiDAR has one big advantage over photogrammetry: it produces its own light.

LiDAR can penetrate the spaces between pieces of foliage and pick up small details.
The laser pulse will see in-between leaves and give a measurement straight to the
tree trunk or ground beneath the tree, whereas photogrammetry depends on photos,
reconstructing only what is visible at the surface.

Photogrammetry benefits from a range of outputs, including colorized point clouds,
textured meshes, and orthomosaics, while LiDAR only produces a point cloud.

* Point Cloud Data Formats                                          :publish:

[[https://info.vercator.com/blog/what-are-the-most-common-3d-point-cloud-file-formats-and-how-to-solve-interoperability-issues][Charles Thomson]][[cite:&Common3DPCFormats]] provides a detail survey about the many 3D point cloud data formats used in the wild.

Almost any general pourpose 3D file format implicitly supports point clouds because those formats usually store a bunch
of polygons, and in order to define a polygon you must define it's vertices, i.e. they just store point cloud data.

** Common file formats

- OBJ ::  a simple ASCII file format[[cite:&enwiki:1087849562]] (though some proprietary binary versions exist).
- OFF ::  a polyhedral surface in Object File Format[[cite:&enwiki:989705529]].
- PLY ::  is a file format[[cite:&enwiki:1092216080]] capable of representing colour, transparency, surface normals, texture,
          coordinates and data confidence values. There are two versions of this file, one in ASCII
          and the other binary.
- XYZ ::  stores Cartesian coordinates in an ASCII format[[cite:&enwiki:1002382601]].
- PCG, RCS, RCP :: file formats developed by Autodesk.
- E57 ::  is a vendor-neutral file format for point cloud storage. It can also be used to store images
          and metadata produced by laser scanners and other 3D imaging systems. It is compact and widely
          used. It also utilises binary code in tandem with ASCII, providing much of the accessibility
          of ASCII and the speed of binary. E57 can represent normals, colours and scalar field intensity.
          The [[https://docs.fileformat.com/3d/e57/][E57]] file format is XML based and was introduced by [[http://paulbourke.net/dataformats/e57/2011-huber-e57-v3.pdf][Daniel Huber]][[cite:&huber_astm_2011]] in 2011
          and it is standardized at [[https://www.astm.org/e2807-11r19.html][ASTM International]]. [[http://www.libe57.org/index.html][LibE57]][[cite:&libE57]] is software tool for managing E57 files.

          The tool [[http://www.libe57.org/data.html][e57xmldump]] is provided by  [[http://www.libe57.org/index.html][LibE57]][[cite:&libE57]]  allowing to extract the /XML/ header from a
          valid /.e57/ file.

         [[https://support.matterport.com/s/article/Overview-of-Matterport-E57-File?language=en_US][ Matterport]] a scanner device company provides also some information about the E57 file format.
- PCD ::  file format used inside [[https://pointclouds.org/documentation/tutorials/pcd_file_format.html][Point Cloud Library (PCL)]][[cite:&enwiki:1069639292]].
- LAS ::  The LAS (LASer) format[[cite:&enwiki:1092274963]] is a file format designed for the interchange and archiving
          of lidar point cloud data. It is an open, binary format specified by the
          American Society for Photogrammetry and Remote Sensing (ASPRS)[[cite:&enwiki:1060954924;&ASPRS2022]].
          The format is widely used and regarded as an industry standard for lidar data.
- HDF5 :: [[https://www.hdfgroup.org/solutions/hdf5/][file format]][[cite:&enwiki:1085523303;&group-1279-hdf5-librar]] used in the context of point cloud
          processing. [[https://www.hdfgroup.org/downloads/hdfview/][HDFView]] is a free /HDF5/ file viewer.

* Point Cloud Viewer Software                                       :publish:

The [[http://www.libe57.org/index.html][LibE57]][[cite:&libE57]] side provides a detailed list of point cloud viewer software. Following a sub list
of viewer software that were tested during writing this article.

** CloudCompare

[[http://www.cloudcompare.org/][CloudCompare]][[cite:&CloudCompare]] is a GPL version 2[[cite:&GPLv2]] based 3D point cloud and mesh processing software.
This program is companioned by a simple viewer application named /ccViewer/.

** Potree

[[https://github.com/potree/potree/][Potree]][[cite:&SCHUETZ-2016-POT]] is a free open-source WebGL based point cloud renderer for large point clouds.

* Point Cloud Gear                                                  :publish:

Three basic technologies exist for producing /point clouds/ in general. Additionally,
combination of technologies are in common use as of today.

- Laser Scanners
- Depth Cameras
- Normal Cameras

Using a /depth sensor/ or going for /photogrammetry/ or even Lidar,  is not an easy choice to make.

In the following some companies and products are mentioned.

** Laser Scanners

*** FARO

[[https://www.faro.com][FARO Europe GmbH]][[cite:&FaroScannerGear2022]] produces high quality laser scanners.
They are available for instance by the [[http://www.3dcad-gmbh.de/3d-laserscan/laserscanner-kauf-miete/3d-laserscan-system-kaufen.html][3D CAD GmbH]]. There is also are marked for
used equipment, e.g. from [[https://3dscannertech.com/used-3d-scanners/used-faro-focus3d-x330][3D SCANNERTECH]].

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: FARO Focus3D-X330 HDR Laser Scanner
#+NAME:   fig:FOCUS-3D-X-330-HDR
[[./FOCUS-3D-X-330-HDR.PNG]]

** Depth Cameras

*** Intel

[[https://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html][Intel]][[cite:&IntelScannerGear2022]] offer a decent assorment of RGB-D cameras like
the [[https://www.intelrealsense.com/depth-camera-d455/][Depth Camera D455]].

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Intel Depth Camera D455
#+NAME:   fig:IntelDepthCameraD455
[[./IntelDepthCameraD455.PNG]]

*** Asus

Asus used to build depth cameras in the past. One example is the [[http://xtionprolive.com/asus-3d-depth-camera/asus-xtion-pro-live][Asus Xtion Pro Live]]
camera.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Asus Xtion Pro Live Depth Camera
#+NAME:   fig:Asus-Xtion-Pro-live
[[./Asus-Xtion-Pro-live.PNG]]

*** Microsoft

Microsoft provides the /Azure Kinect DK/ is a developer kit[[cite:&kintecDK]] for
for building advanced computer vision and speech models.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Azure Kintec DK
#+NAME:   fig:Azure-Kintec-DK
[[./AzureKinectDK.PNG]]

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Azure Kintec DK
#+NAME:   fig:Azure-Kintec-DK
[[./AzureKinectDK-1.PNG]]

*** Normal Cameras

Photorametry[[cite:&enwiki:1091964951;&enwiki:1060954924]] is the science and technology of
obtaining reliable information about physical objects and the environment through the
process of recording, measuring and interpreting photographic images and patterns of
electromagnetic radiant imagery and other phenomena.

Basically, triangulation is used in the case of normal camera point cloud generation.

** Modern MultiSensor Scanners

*** NavVis

The german company [[https://www.navvis.com/][NaVis]] produces wearable mobile multi sensor scanning devices.

NavVis enables service providers and enterprises to capture and share the built environment as
photorealistic digital twins. Our SLAM-based mobile mapping systems generate high-quality data
with survey-grade accuracy at speed and scale.

Based in Munich, Germany, with offices in the United States and China, NavVis has customers
worldwide in the surveying, AEC, and manufacturing industries.

A really informative youtube videos are available:
- [[https://www.youtube.com/watch?v=4LmwNYtLp_o][INTERGEO 2021: NavVis VLX live demonstration]]
- [[https://www.youtube.com/watch?v=LpUe6V3b_D0][INTERGEO 2021: NavVis VLX live demonstration]]
- [[https://www.youtube.com/watch?v=Ezsqcwrx4dg][NavVis VLX Demo]]
- [[https://www.youtube.com/watch?v=cuf4VGAt1X4][Captured with NavVis: Scanning a 500 year-old Norwegian farmhouse | Alver, NO]]
- [[https://www.youtube.com/watch?v=ZP1dMHDudBE][Captured with NavVis: Stockholm City Hall | Stockholm, SE]]
- [[https://www.youtube.com/watch?v=pS-ZxLWamjQ][Captured with NavVis: Monticchiello | Tuscany, IT]]
- [[https://www.youtube.com/watch?v=qAXgtQJVA98][Captured with NavVis: Seligenstadt Abbey | Seligenstadt, DE]]

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][NavVis VLX 2]{
  \includegraphics[width=.37\linewidth]{./NavVis1.PNG}
}
\qquad
\subfloat[][Mobile scanner]{
  \includegraphics[width=.25\linewidth]{./NavVis2.PNG}
}
\qquad
\subfloat[][NavVis VLX 2]{
  \includegraphics[width=.5\linewidth]{./NavVis0.PNG}
}
\caption{Navis MultiSensor Scanners}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: NavVis VLX 2 scanner
#+NAME:   fig:NavVis1
[[./NavVis1.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: Navis mobile scanner
#+NAME:   fig:NavVis2
[[./NavVis2.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: NavVis VLX 2 scanner
#+NAME:   fig:NavVis1
[[./NavVis0.PNG]]

#+end_export

Access data to DataSolid NavVis-Scan with NavVis VLX 2 Scanner:

#+begin_example
URL: NavVis IVION
Name: Datasolid
PW: NavVis@2022
#+end_example

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Office]{
  \includegraphics[width=.4\linewidth]{./NavVisDS-0.PNG}
}
\qquad
\subfloat[][Office details]{
  \includegraphics[width=.4\linewidth]{./NavVisDS-1.PNG}
}
\qquad
\subfloat[][DataSolid GmbH: First floor]{
  \includegraphics[width=.8\linewidth]{./NavVisDS-2.PNG}
}
\caption{Navis MultiSensor Scanners}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: Office
#+NAME:   fig:NavVisDS-0
[[./NavVisDS-0.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: Office details
#+NAME:   fig:NavVisDS-1
[[./NavVisDS-1.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 220px
#+CAPTION: DataSolid GmbH: First floor
#+NAME:   fig:NavVisDS-2
[[./NavVisDS-2.PNG]]

#+end_export

Price of the NavVis VLX scanner:

#+begin_example
Hardware:         ~60.000€
Annual Cloud:      ~5.000€
Panorama per case: ~1.500€
#+end_example

Contact data to NavVis:

#+begin_example
Harald Saeger
Senior Account Executive Laserscanning DACH

+49 151 11856157
harald.saeger@navvis.com
www.navvis.com

Blutenburgstr. 18, 80636 Munich, Germany
Amtsgericht München l Handelsregisternummer HRB 205407

Managing directors: Dr. Felix Reinshagen, Dr. Georg Schroth, Finn Boysen, Jeno Schadrack
#+end_example




[[https://www.3dscan-solutions.de][3DScan Solution]] is a german scanning service provider with an office in the city of Düsseldorf.
They have the NavVis VLX 2 scanner their device portofolio.

#+begin_example
Hauptniederlassung:
Johann-Sedlmeir-Str. 7b
86919 Utting am Ammersee, Deutschland
Telefon: +49-8806-92890-50
Telefax: +49-8806-92890-55

Vertriebsbüro Düsseldorf:
Speditionstrasse 21
40221 Düsseldorf, Deutschland
Telefon: +49-0211-542089-20
Telefax: +49-0211-542089-29

Email: support@3dscan-solutions.de

Geschäftsführer: Frank Lemm
Amtsgericht: Augsburg | HRB: 35342
Steuer-Nr.: 125/124/90245
3DScan Solution
#+end_example

*** Flyability Elios 3

[[https://www.flyability.com/elios-3][Flyability Elios 3]] is an indoor multi-sensor drone.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Flyability Elios 3
#+NAME:   fig:Elios3
[[./Elios3.PNG]]

** Equipment

*** Usables Düsseldorf

[[https://www.kamera-verleih-duesseldorf.de/][Usables Düsseldorf]] is a camera and equipement rental service located in the city of Düsselforf.

*** Grover Berlin

[[https://www.grover.com/de-de][Grover Deutschland GmbH]] is a camera and equipement rental service located in the city of Berlin.

* Point Cloud Libraries                                             :publish:

A number of /Open-Source/ point cloud libraries are available. Ivan Nikolov[[cite:&nikolov_python_2022_1;&nikolov_python_2022-2]]
gives a detailed comparsion of some Python[[cite:&Python]] libraries for 3D point cloud processing.

A general overview about common /Open Source/ license models can be found on Wikipedia[[cite:&enwiki:1090257374]].

** CGAL - Computational Geometry Algorithms Library

The [[https://www.cgal.org/][Computational Geometry Algorithms Library (CGAL)]][[cite:&CGAL]] is one of the most versatile /C++ libraries/ in the field
providing a rich set of dedicated [[https://doc.cgal.org/latest/Manual/packages.html][packages]]. CGAL is distributed under a [[https://doc.cgal.org/latest/Manual/license.html][dual license scheme]], that is under the GNU GPL/LGPL
open source licenses, as well as under commercial licenses. Users who cannot comply with the Open Source license terms can
buy individual data structures under various commercial licenses from [[https://geometryfactory.com/][Geometric Factory]].

The CGAL library contains a [[https://doc.cgal.org/latest/Manual/tuto_reconstruction.html][Surface Reconstruction from Point Clouds]] tutorial. The following image shows an overview of
the pipeline used by CGAL.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Pipeline Overview
#+NAME:   fig:CGAL01
[[./ReconstructionPipeline.png]]

** PCL - Point Cloud Library

The [[https://pointclouds.org/][Point Cloud Library (PCL)]][[cite:&Rusu_ICRA2011_PCL]] is a standalone, large scale, open project for 2D/3D
image and point cloud processing. PCL is released under the terms of the BSD license[[cite:&enwiki:1091052504]],
and thus free for commercial and research use.

The [[https://pointclouds.org/documentation/][PCL framework]] contains numerous state-of-the art algorithms including filtering,
feature estimation, surface reconstruction, registration, model fitting and segmentation.

PCL is a cross-platform /C++ library/, and has been successfully compiled and deployed on Linux, MacOS, Windows, and Android.
[[https://pcl.readthedocs.io/projects/tutorials/en/master/][Tutorials]] are available on /readthedocs/.

*** Installation

The [[https://pointclouds.org/][Point Cloud Library (PCL)]] is available for installation by the [[https://github.com/microsoft/vcpkg][vcpkg]] C++ package manager. The installation is performed
on the command line:

- Prerequisites: [[https://git-scm.com/download/win][Git]] and [[https://cmake.org/download/][CMake]]
- Installation of [[https://github.com/microsoft/vcpkg][vcpkg]]:

  #+begin_src shell
mkdir d:/work/vcpkg
cd d:/work/vcpkg

git clone https://github.com/microsoft/vcpkg.git .
bootstrap-vcpkg.bat  -disableMetrics

# vcpkg install pcl[*]:x64-windows --recurse

vcpkg install pcl[apps,examples,qt,simulation,\
                  tools,visualization,vtk]:x64-windows \
              --recurse

vcpkg install boost:x64-windows
  #+end_src

- Additional simple overview of vcpkg can be found [[https://thatonegamedev.com/cpp/how-to-manage-dependencies-with-cmake-and-vcpkg/][here]].

**** Updating and Upgrading

In case that you have to upgrade [[https://github.com/microsoft/vcpkg][vcpkg]] run the following commands from the bash command line:

#+begin_src shell
cd d:/work/vcpkg

git pull & run bootstrap-vcpkg.bat
#+end_src

In case that you want to update a specific library do the following:

#+begin_src shell
vcpkg upgrade zlib
vcpkg upgrade zlib:x86-windows
#+end_src

In case that you want to update all installed libraries do the following:

#+begin_src shell
vcpkg upgrade
vcpkg upgrade --no-dry-run
#+end_src

*** Validating Installation

The companying source code to the paper [[https://arxiv.org/pdf/1905.02553.pdf][Oriented Point Sampling for Plane Detection in Unorganized Point
Clouds]][[cite:&Sun2019]] is available on [[https://github.com/victor-amblard/OrientedPointSampling][Github]] and is implemented on top of [[https://pointclouds.org/][PCL]]. The following is the recipe
to compile this code.

#+begin_src shell
mkdir -p d:/work/pcl/OrientedPointSampling
cd d:/work/pcl/OrientedPointSampling

git clone https://github.com/victor-amblard/OrientedPointSampling.git .

mkdir build

cmake -B ./build -S . -G "Visual Studio 16 2019" \
      -A "x64" -T "v142" \
      -DCMAKE_TOOLCHAIN_FILE=\
  d:/work/vcpkg/scripts/buildsystems/vcpkg.cmake
#+end_src

After that the file =orientedPointSampling.sln= can be opened and compiled.


*** Python - Wrapper

**** python-pcl

[[https://github.com/strawlab/python-pcl][python-pcl]] is a /Cython/ based wrapper of /PCL/ that is only available for Python version $<= 3.6$.

The last code change happens around 2019.

**** pclpy

[[https://github.com/davidcaron/pclpy][pclpy]] is a /pybind11/ based wrapper of /PCL/ that is only available for Python version $<= 3.7$.

The following command install the $3.6$ version in an /Anaconda Environment/ based on Python $3.6$.

#+begin_src shell
conda install -c conda-forge -c davidcaron pclpy
#+end_src

**** pcl.py

[[https://github.com/cmpute/pcl.py][pcl.py]] is another /Cython/ based /PCL/ wrapper that is not provided as a wheel and that could not
be installed by the author.

** Open3D

[[http://www.open3d.org/][Open3D]][[cite:&open3d]] is an open-source library released under the MIT license[[cite:&enwiki:1091025780]] that supports rapid
development of software that deals with 3D data. The publication[[cite:&Zhou2018]] of Zhou et. all provides an overview of
the library and a comparsion to the [[https://pointclouds.org/][Point Cloud Library (PCL)]][[cite:&Rusu_ICRA2011_PCL]] library. Open3D is a /C++ library/
that also provides a Python[[cite:&Python]] interface.

*** Installation

The newest /Python/ version that the /Open3D/ library did support at the time of writing this document was /Python 3.9/.
Installation can be easily performed by running the /Python/ package manager /pip/. This will also install all dependencies
packages of /Open3D/. It is therefore  recommended to use a /Python virtual environment/.

#+begin_src shell
python -m pip install open3d
python -c "import open3d as o3d; print(o3d.__version__)" # 0.15.1
#+end_src

Test the /Open3D/ library. The first version opens the standalone visualizer. Unfortunately, does this not work
on [[https://github.com/isl-org/Open3D/issues/4852][AMD based graphic platforms]].

#+begin_src python
import open3d as o3d
mesh = o3d.geometry.TriangleMesh.create_sphere()
mesh.compute_vertex_normals()
mesh.paint_uniform_color((1.0, 0.0, 0.0))
o3d.visualization.draw(mesh, raw_mode=True)
#+end_src

Alternatively, /Open3D/ provides a browser based visualizer that seems to work fine and
can be started by opening the following link in your browser [[http://localhost:8888]].

#+begin_src python
import open3d as o3d
o3d.visualization.webrtc_server.enable_webrtc()
mesh = o3d.geometry.TriangleMesh.create_sphere()
mesh.compute_vertex_normals()
mesh.paint_uniform_color((1.0, 0.0, 0.0))
o3d.visualization.draw(mesh)
#+end_src

*** Open3D for TensorBoard

[[https://www.tensorflow.org/tensorboard?hl=en][TensorBoard]] is [[https://www.tensorflow.org/?hl=en][TensorFlow's]][[cite:&tensorflow2015-whitepaper]] visualization toolkit and provides the visualization
and tooling needed for machine learning experimentation. Install /TensorFlow/ with /pip/.

#+begin_src shell
pip install tensorflow

python -c "\
  import tensorflow as tf; \
  print(tf.reduce_sum(tf.random.normal([1000, 1000])))"

python -c "\
  import tensorflow as tf;
  print(tf.config.list_physical_devices('GPU'))"
#+end_src

/TensorFlow/ requires a /CUDA/ capable /GPU/ for proper functioning. Additionally, the [[https://pytorch.org/][PyTorch]][[cite:&PyTorch]] package is
required and can be installed as follows:

#+begin_src shell
pip install torch
#+end_src

**** Example 1: Simple geometry sequences

Save and visualize simple geometry data such as a cube and cylinder.

Both /PyTorch/ and /TensorFlow/ are supported by /Open3D/. The first example uses the
corresponding /PyTorch/ code.

#+begin_src python
import open3d as o3d

from open3d.visualization.tensorboard_plugin \
    import summary
from open3d.visualization.tensorboard_plugin.util \
    import to_dict_batch

from torch.utils.tensorboard import SummaryWriter

if  __name__=='__main__':
    #
    # Create some simple geometry data:
    #
    cube = o3d.geometry.TriangleMesh.create_box(
        1, 2, 4)
    cube.compute_vertex_normals()
    cylinder = o3d.geometry.TriangleMesh.create_cylinder(
        radius=1.0,
        height=2.0,
        resolution=20,
        split=4)
    cylinder.compute_vertex_normals()
    colors = [
        (1.0, 0.0, 0.0),
        (0.0, 1.0, 0.0),
        (0.0, 0.0, 1.0)
    ]

    #
    # Now lets write this as a summary:
    #
    logdir = "demo_logs/pytorch/small_scale"
    writer = SummaryWriter(logdir)
    for step in range(3):
        cube.paint_uniform_color(
            colors[step])
        writer.add_3d(
            'cube',
            to_dict_batch([cube]),
            step=step)
        cylinder.paint_uniform_color(
            colors[step])
        writer.add_3d(
            'cylinder',
            to_dict_batch([cylinder]),
            step=step)
#+end_src

#+begin_src shell
tensorboard --logdir demo_logs/pytorch
#+end_src

And the second example uses the corresponding /Tensorflow/ code.

#+begin_src python
import open3d as o3d

from open3d.visualization.tensorboard_plugin \
    import summary
from open3d.visualization.tensorboard_plugin.util \
    import to_dict_batch

import tensorflow as tf

if  __name__=='__main__':
    #
    # Create some simple geometry data:
    #
    cube = o3d.geometry.TriangleMesh.create_box(
        1, 2, 4)
    cube.compute_vertex_normals()
    cylinder = o3d.geometry.TriangleMesh.create_cylinder(
        radius=1.0,
        height=2.0,
        resolution=20,
        split=4)
    cylinder.compute_vertex_normals()
    colors = [
        (1.0, 0.0, 0.0),
        (0.0, 1.0, 0.0),
        (0.0, 0.0, 1.0)
    ]

    #
    # Now lets write this as a summary:
    #
    logdir = "demo_logs/tf/small_scale"
    writer = tf.summary.create_file_writer(
        logdir)
    with writer.as_default():
    for step in range(3):
        cube.paint_uniform_color(
            colors[step])
        summary.add_3d(
            'cube',
            to_dict_batch([cube]),
            step=step,
            logdir=logdir)
        cylinder.paint_uniform_color(
            colors[step])
        summary.add_3d(
            'cylinder',
            to_dict_batch([cylinder]),
            step=step,
            logdir=logdir)
#+end_src

#+begin_src shell
tensorboard --logdir demo_logs/tf
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Tensorboard example
#+NAME:   fig:tensorBoard1
[[./tensorboard.png]]

** PyntCloud

[[https://github.com/daavoo/pyntcloud][PyntCloud]] is a Python 3 library for working with 3D point clouds leveraging the power of the Python scientific stack.

With PyntCloud you can perform complex 3D processing operations with minimum lines of code.


- Load a PLY point cloud from disk.
- Add 3 new scalar fields by converting RGB to HSV.
- Build a grid of voxels from the point cloud.
- Build a new point cloud keeping only the nearest point to each occupied voxel center.
- Save the new point cloud in numpy's NPZ format.

#+caption: PyntCloud example.
#+begin_src python
from pyntcloud import PyntCloud

cloud = PyntCloud.from_file("some_file.ply")

cloud.add_scalar_field("hsv")

voxelgrid_id = cloud.add_structure(
    "voxelgrid",
    n_x = 32,
    n_y = 32,
    n_z = 32)

new_cloud = cloud.get_sample(
    "voxelgrid_nearest",
    voxelgrid_id = voxelgrid_id,
    as_PyntCloud = True)

new_cloud.to_file("out_file.npz")
#+end_src

pyntcloud offers seamless integration with other 3D processing libraries.

#+caption: PyntCloud integration with Open3D.
#+begin_src python
import open3d as o3d
from pyntcloud import PyntCloud

# FROM Open3D
original_triangle_mesh = \
    o3d.io.read_triangle_mesh("diamond.ply")

cloud = \
    PyntCloud.from_instance(
        "open3d",
        original_triangle_mesh)

# TO Open3D
cloud = PyntCloud.from_file(
    "diamond.ply")
converted_triangle_mesh = \
    cloud.to_instance(
        "open3d",
        mesh = True)
#+end_src

#+caption: PyntCloud integration with pyVista.
#+begin_src python
import pyvista as pv
from pyntcloud import PyntCloud

# FROM PyVista
original_point_cloud = pv.read(
    "diamond.ply")

cloud = PyntCloud.from_instance(
    "pyvista",
    original_point_cloud)

# TO PyVista
cloud = PyntCloud.from_file(
    "diamond.ply")

converted_triangle_mesh = \
    cloud.to_instance(
        "pyvista",
        mesh = True)
#+end_src

** PCU -Point Cloud Utils

[[https://github.com/fwilliams/point-cloud-utils][Point Cloud Utils (PCU)]] is a utility library providing the following
functionality for 3D processing point clouds and triangle meshes.


- Utility functions for reading and writing many common mesh formats
  (PLY, STL, OFF, OBJ, 3DS, VRML 2.0, X3D, COLLADA).

- A series of algorithms for generating point samples on meshes:
  - Poisson-Disk-Sampling of a mesh based on
     [[http://graphics.cs.umass.edu/pubs/sa_2010.pdf][Parallel Poisson Disk Sampling with Spectrum Analysis on Surface]].
  - Sampling a mesh with [[https://en.wikipedia.org/wiki/Lloyd%27s_algorithm][Lloyd's algorithm]].
  - Monte-Carlo sampling on a mesh.
- Utilities for downsampling point clouds:
  - To satisfy a blue noise distribution
  - On a voxel grid
- Closest points between a point cloud and a mesh
- Normal estimation from point clouds and triangle meshes
- Fast k-nearest-neighbor search between point clouds (based on nanoflann).
- Hausdorff distances between point-clouds.
- Chamfer distances between point-clouds.
- Approximate Wasserstein distances between point-clouds using the Sinkhorn method.
- Compute signed distances between a point cloud and a mesh using Fast Winding Numbers
- Compute closest points on a mesh to a point cloud
- Deduplicating point clouds and mesh vertices
- Fast ray/mesh intersection using embree
- Fast ray/surfel intersection using embree
- Mesh smoothing
- Mesh connected components
- Mesh decimation
- Removing duplicate/unreferenced vertices in point clouds and meshes
- Making a mesh watertight (based on the Watertight Manifold algorithm)

** Polylidar3D

[[https://github.com/JeremyBYU/polylidar][Polylidar3D]][[cite:&s20174819]] is a non-convex polygon extraction algorithm which takes as input either unorganized 2D point sets,
unorganized 3D point clouds. The [[https://jeremybyu.github.io/polylidar/][library]] is written in /C++/ that can also be used from /Python/.

#+begin_src shell
pip install polylidar
#+end_src

** Pye57

The [[https://github.com/davidcaron/pye57][pye57]] library is a wrapper of [[https://github.com/asmaloney/libE57Format][libE57Format]] /C++/ library that allows to read and write /.e57/ point cloud files.
Neither the [[https://www.cgal.org/][CGAL]][[cite:&CGAL]], the [[https://pointclouds.org/][PCL]][[cite:&Rusu_ICRA2011_PCL]], nor the [[http://www.open3d.org/][Open3D]][[cite:&open3d]] handles the /.e57/ file format
natively.

** Plyfile

The [[https://github.com/dranjan/python-plyfile][plyfile]] library provides a simple facility for reading and writing ASCII and binary PLY files.

#+caption: Test PLY file tet.ply
#+begin_example
ply
format ascii 1.0
comment single tetrahedron with colored faces
element vertex 4
comment tetrahedron vertices
property float x
property float y
property float z
element face 4
property list uchar int vertex_indices
property uchar red
property uchar green
property uchar blue
end_header
0 0 0
0 1 1
1 0 1
1 1 0
3 0 1 2 255 255 255
3 0 2 3 255 0 0
3 0 1 3 0 255 0
3 1 2 3 0 0 255
#+end_example

#+begin_src python
from plyfile import PlyData, PlyElement

plydata = PlyData.read('tet.ply')

with open('tet.ply', 'rb') as f:
    plydata = PlyData.read(f)

plydata.elements[0].name
# 'vertex'

plydata.elements[0].data[0]
# (0.0, 0.0, 0.0)

plydata['vertex'][0]
# (0.0, 0.0, 0.0)

plydata['vertex'].data[0]
# (0.0, 0.0, 0.0)

plydata.elements[0].data['x']
# array([ 0.,  0.,  1.,  1.], dtype=float32)

plydata['vertex']['x']
# array([ 0.,  0.,  1.,  1.], dtype=float32)

plydata['face'].data['vertex_indices'][0]
# array([0, 1, 2], dtype=int32)

plydata.elements[0].properties
# (PlyProperty('x', 'float'),
#  PlyProperty('y', 'float'),
#  PlyProperty('z', 'float'))

plydata.elements[0].count
# 4

vertex = numpy.array(
    [(0, 0, 0),
     (0, 1, 1),
     (1, 0, 1),
     (1, 1, 0)],
    dtype=[('x', 'f4'),
           ('y', 'f4'),
           ('z', 'f4')])

 face = numpy.array(
     [([0, 1, 2], 255, 255, 255),
      ([0, 2, 3], 255,   0,   0),
      ([0, 1, 3],   0, 255,   0),
      ([1, 2, 3],   0,   0, 255)],
     dtype=[('vertex_indices', 'i4', (3,)),
            ('red',   'u1'),
            ('green', 'u1'),
            ('blue',  'u1')])

el = PlyElement.describe(some_array, 'some_name')
el = PlyElement.describe(some_array, 'some_name',
                         comments=['comment1',
                                   'comment2'])

el = PlyElement.describe(some_array, 'some_name',
                         val_dtypes={'some_property': 'f8'},
                         len_dtypes={'some_property': 'u4'})

PlyData([el]).write('some_binary.ply')
PlyData([el], text=True).write('some_ascii.ply')
PlyData([el], byte_order='>').write('some_big_endian_binary.ply')

with open('some_ascii.ply', mode='wb') as f:
    PlyData([el], text=True).write(f)
#+end_src

*** Write hdf5 file from ply files

Simply use the /plyfile/ tool to fetch the points from /.ply/ files as
/numpy array/, and then write these arrays into /HDF5/ file.

Concretely speaking, when writing an /HDF5/ file, we first read the
raw data from the $N$ /point clouds/ as a 3D array $(N\times 2048\times 3)$,
then write the array to the /HDF5/ dataset.

The /.h5/ files should contain /2 datasets/ - one for the /data/ itself and one
for the /lables/. In the /data/ dataset each row (and its depth) is one point cloud,
meaning the /data/ dataset has dimensions $N\times P \times 3$ where $N$ is the
number of point clouds in the file, $P$ is the number of points in a single point
cloud, and $3$ is because each point has $x,y,z$ coordinates.

#+begin_src python
import h5py
import numpy as np
from plyfile import PlyData, PlyElement

filenames = [line.rstrip() for line in open("filelist", 'r')]

#f = h5py.File("./hdf5_data/data_training.h5", 'w')
f = h5py.File("./hdf5_data/data_testing.h5", 'w')

a_data = np.zeros((len(filenames), 2048, 3))
a_pid  = np.zeros((len(filenames), 2048), dtype = np.uint8)

for i in range(0, len(filenames)):
    plydata = PlyData.read("./ply/" + filenames[i] + ".ply")
    piddata = [
        line.rstrip() for line in open(
            "./points_label/" + filenames[i] + ".seg", 'r'
        )
    ]

    for j in range(0, 2048):
	      a_data[i, j] = [
                  plydata['vertex']['x'][j],
                  plydata['vertex']['y'][j],
                  plydata['vertex']['z'][j]]

        a_pid[i,j] = piddata[j]

data = f.create_dataset("data", data = a_data)
pid  = f.create_dataset("pid",  data = a_pid )
#+end_src

** PDAL - Point Data Abstraction Library

The [[https://pdal.io/][PDAL - Point Data Abstraction Library]][[cite:&PDAL]] is a /C++ library/ for translating and manipulating point cloud data[[cite:&enwiki:1086318007]].
It can be found on [[https://github.com/PDAL/PDAL][Github.]]

PDAL allows you to compose operations on point clouds into pipelines of stages. These pipelines can be written in a declarative /JSON/
syntax or constructed using the available API.

In addition a /Python/ wrapper is available for PDAL.

*** PDAL Python

[[https://github.com/PDAL/python][PDAL python]] and the extension [[https://github.com/PDAL/python-plugins][PDAL python plugins]] can be installed via /pip/ in the usual way.

PDAL Python support allows you to process data with PDAL into Numpy arrays. It provides a PDAL extension module to
control Python interaction with PDAL. Additionally, you can use it to fetch schema and metadata from PDAL operations.

#+begin_src shell
pip install pdal
pip install pdal-plugins
#+end_src

This didn't work out of the box with the standard /Python/ installation that was used for testing.
However, a [[https://docs.conda.io/en/latest/][Conda]] based installation on [[https://github.com/conda-forge/miniforge][Miniforge]] did work.

#+begin_src shell
conda install -c conda-forge python-pdal
#+end_src

*** PDAL Command

Basically, a [[https://pdal.io/][PDAL]][[cite:&PDAL]] installation on [[https://github.com/conda-forge/miniforge][Miniforge]] installs the /pdal.exe/ command line tool.

#+begin_src shell
conda install -c conda-forge pdal
#+end_src

The /pdal.exe/ command can be used as follows:

#+begin_src shell
pdal.exe --help
pdal.exe info C:\Users\hobu\Downloads\autzen.laz -p 0
pdal.exe translate interesting.las interesting.txt
#+end_src

* Point Cloud Model /Brotfabrik.e57/                                  :publish:

The model *Brotfabrik.e57* constists of about 11 millions of 3D points with RBG color data attached to the points.

** Reading the XML header

The tool [[http://www.libe57.org/data.html][e57xmldump]] allows the extraction of the meta data contained in a valid /.e57/ file.

#+begin_src shell
e57xmldump.exe  Brotfabrik.e57 > Brotfabrik.xml
#+end_src

#+begin_src xml
<?xml version="1.0" encoding="UTF-8"?>
<e57Root type="Structure"
         xmlns="http://www.astm.org/COMMIT/E57/2010-e57-v1.0">
  <formatName type="String">
    <![CDATA[ASTM E57 3D Imaging Data File]]>
  </formatName>
  <guid type="String">
    <![CDATA[{7255AA8C-B05B-4855-B1FE-947667D03022}]]>
  </guid>
  <versionMajor type="Integer">1</versionMajor>
  <versionMinor type="Integer"/>
  <e57LibraryVersion type="String">
    <![CDATA[unknown]]>
  </e57LibraryVersion>
  <coordinateMetadata type="String"/>
  <creationDateTime type="Structure">
    <dateTimeValue type="Float">1.3387e+09</dateTimeValue>
    <isAtomicClockReferenced type="Integer"/>
  </creationDateTime>
  <data3D type="Vector" allowHeterogeneousChildren="1">
    <vectorChild type="Structure">
      <guid type="String"><![CDATA[0000000000000001]]></guid>
      <sensorSoftwareVersion type="String">
        <![CDATA[Leica Geosystems]]>
      </sensorSoftwareVersion>
      <intensityLimits type="Structure">
        <intensityMaximum type="Float">1.0e+00</intensityMaximum>
        <intensityMinimum type="Float"/>
      </intensityLimits>
      <colorLimits type="Structure">
        <colorRedMaximum   type="Integer">255</colorRedMaximum>
        <colorRedMinimum   type="Integer"/>
        <colorGreenMaximum type="Integer">255</colorGreenMaximum>
        <colorGreenMinimum type="Integer"/>
        <colorBlueMaximum  type="Integer">255</colorBlueMaximum>
        <colorBlueMinimum  type="Integer"/>
      </colorLimits>
      <cartesianBounds type="Structure">
        <xMinimum type="Float">-1.4754e01</xMinimum>
        <xMaximum type="Float">-1.2288e+01</xMaximum>
        <yMinimum type="Float">-3.0715e+00</yMinimum>
        <yMaximum type="Float">-5.0000e-04</yMaximum>
        <zMinimum type="Float">-1.6195e+00</zMinimum>
        <zMaximum type="Float">-5.0000e-04</zMaximum>
      </cartesianBounds>
      <pose type="Structure">
        <rotation type="Structure">
          <w type="Float">1.0e+00</w>
          <x type="Float"/>
          <y type="Float"/>
          <z type="Float"/>
        </rotation>
        <translation type="Structure">
          <x type="Float"/>
          <y type="Float"/>
          <z type="Float"/>
        </translation>
      </pose>
      <points type="CompressedVector"
              fileOffset="48"
              recordCount="11136824">
        <prototype type="Structure">
          <cartesianX type="ScaledInteger"
                      minimum="-2147483647"
                      maximum="2147483647"
                      scale="1.0e-05"/>
          <cartesianY type="ScaledInteger"
                      minimum="-2147483647"
                      maximum="2147483647"
                      scale="1.0e-05"/>
          <cartesianZ type="ScaledInteger"
                      minimum="-2147483647"
                      maximum="2147483647"
                      scale="1.0e-05"/>
          <intensity  type="Float"
                      precision="single"
                      minimum="0.0000000e+00"
                      maximum="1.0000000e+00"/>
          <colorRed   type="Integer" minimum="0" maximum="255"/>
          <colorGreen type="Integer" minimum="0" maximum="255"/>
          <colorBlue  type="Integer" minimum="0" maximum="255"/>
        </prototype>
        <codecs type="Vector" allowHeterogeneousChildren="1">
        </codecs>
      </points>
    </vectorChild>
  </data3D>
  <images2D type="Vector" allowHeterogeneousChildren="1">
  </images2D>
</e57Root>
#+end_src

** Utilizing PDAL

The [[https://pdal.io/][PDAL - Point Data Abstraction Library]][[cite:&PDAL]] command line tool /pdal.exe/ can be used for many task.
We use it here to show some simple data extraction and file type conversion of the /Brotfabrik.e57/ model.

*** Extracting Points

The following command extracts three points from the point cloud file.

#+begin_src shell
pdal.exe info Brotfabrik.e57 -p 0-3
#+end_src

#+begin_src example
{
  "file_size": 212518912,
  "filename": "Brotfabrik.e57",
  "now": "2022-06-24T09:17:07+0200",
  "pdal_version": "2.4.2 (git-version: Release)",
  "points":
  {
    "point":
    [
      {
        "Blue": 41634,
        "Green": 41634,
        "Intensity": 21425,
        "PointId": 0,
        "Red": 41634,
        "X": -14.6035,
        "Y": -2.6915,
        "Z": -1.6055
      },
      {
        "Blue": 43947,
        "Green": 43947,
        "Intensity": 24264,
        "PointId": 1,
        "Red": 43947,
        "X": -14.5975,
        "Y": -2.7205,
        "Z": -1.5965
      },
      {
        "Blue": 37265,
        "Green": 37265,
        "Intensity": 16204,
        "PointId": 2,
        "Red": 37265,
        "X": -14.6315,
        "Y": -2.6895,
        "Z": -1.5965
      },
      {
        "Blue": 43690,
        "Green": 43690,
        "Intensity": 23813,
        "PointId": 3,
        "Red": 43690,
        "X": -14.6095,
        "Y": -2.7065,
        "Z": -1.5985
      }
    ]
  },
  "reader": "readers.e57"
}
#+end_src

*** File Type Conversion

The following command converts the /Brotfabrik.e57/ file into a simple text file. Only the actual data points are
extracted.

#+begin_src shell
pdal.exe translate Brotfabrik.e57 Brotfabrik.txt
#+end_src

#+begin_src example
"X","Y","Z","Red","Green","Blue","Intensity"
-14.604,-2.692,-1.606,41634.000,41634.000,41634.000,21425.000
-14.598,-2.721,-1.597,43947.000,43947.000,43947.000,24264.000
...
#+end_src

And finally, the conversion into some of the widely used file formats.

#+begin_src shell
pdal.exe translate Brotfabrik.e57 Brotfabrik.xyz
pdal.exe translate Brotfabrik.e57 Brotfabrik.ply
pdal.exe translate Brotfabrik.e57 Brotfabrik.las
#+end_src

*** Meta Information Extraction

The following command extracts meta information from the point cloud file.

#+begin_src shell
pdal.exe info --metadata Brotfabrik.e57
#+end_src

#+begin_src example
{
  "file_size": 212518912,
  "filename": "Brotfabrik.e57",
  "metadata":
  {
  },
  "now": "2022-06-24T08:28:03+0200",
  "pdal_version": "2.4.2 (git-version: Release)",
  "reader": "readers.e57"
}
#+end_src

** Visualizing with ccViewer and CloudCompare

[[http://www.cloudcompare.org/][CloudCompare]][[cite:&CloudCompare]] is a GPL version 2[[cite:&GPLv2]] based 3D point cloud and mesh processing software.
This program is companioned by a simple viewer application named /ccViewer/.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Brotfabrik.e57 loaded by ccViewer
#+NAME:   fig:ccViewer1
[[./ccViewer1.png]]

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Brotfabrik.e57 loaded by CloudCompare
#+NAME:   fig:CloudCompare1
[[./CloudCompare1.png]]

** Visualizing with Potree

[[https://github.com/potree/PotreeDesktop/releases][PotreeDesktop]] is a [[https://github.com/potree/potree][Potree]] based desktop viewer that is able to load /.las/ files.

The following picture is taken from the PotreeDesktop with the /Brotfabrik.las/ file
created with /pdal.exe/.

#+begin_src shell
pdal.exe translate Brotfabrik.e57 Brotfabrik.las
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Brotfabrik.e57 loaded by PotreeDesktop
#+NAME:   fig:Potree1
[[./Potree1.PNG]]

** Open3D Experiments

In the following some experiments utilizing the [[http://www.open3d.org/][Open3D]][[cite:&open3d]] library are shown.

*** Utilizing the PyE57 Library

The [[https://github.com/davidcaron/pye57][pye57]] library allows us to read /.e57/ files. Following a boilerplate script for reading
/.e57/ files into a [[https://numpy.org/][NumPy]] array.

#+begin_src python
import numpy as np
import pye57

e57 = pye57.E57("b:\PointClouds\Data\Brotfabrik.e57")

# read scan at index 0
data = e57.read_scan(0, ignore_missing_fields=True)

# 'data' is a dictionary with the point types as keys
assert isinstance(data["cartesianX"], np.ndarray)
assert isinstance(data["cartesianY"], np.ndarray)
assert isinstance(data["cartesianZ"], np.ndarray)

# other attributes can be read using:
data = e57.read_scan(
    0,
    intensity=True,
    colors=True,
    row_column=True,
    ignore_missing_fields=True)

assert isinstance(data["cartesianX"], np.ndarray)
assert isinstance(data["cartesianY"], np.ndarray)
assert isinstance(data["cartesianZ"], np.ndarray)
assert isinstance(data["intensity" ], np.ndarray)
assert isinstance(data["colorRed"  ], np.ndarray)
assert isinstance(data["colorGreen"], np.ndarray)
assert isinstance(data["colorBlue" ], np.ndarray)
# assert isinstance(data["rowIndex"   ], np.ndarray) # n/a
# assert isinstance(data["columnIndex"], np.ndarray) # n/a

# the 'read_scan' method filters points using the
# 'cartesianInvalidState' field. If you want to get
# everything as raw, untransformed data, use:
data_raw = e57.read_scan_raw(0)

# writing is also possible, but only using raw data for now
e57_write = pye57.E57("e57_file_write.e57", mode='w')
e57_write.write_scan_raw(data_raw)

# the ScanHeader object wraps most of the scan information:
header = e57.get_header(0)

print(header.point_count)
print(header.rotation_matrix)
print(header.translation)

# all the header information can be printed using:
for line in header.pretty_print():
    print(line)

# the scan position can be accessed with:
position_scan_0 = e57.scan_position(0)

# the binding is very close to the E57Foundation API
# you can modify the nodes easily from python
imf = e57.image_file
root = imf.root()
data3d = root["data3D"]
scan_0 = data3d[0]
translation_x = scan_0["pose"]["translation"]["x"]
#+end_src

*** Visualizing the Brotfabrik.e57

Visuzalize the /Brotfabrik.e57/ model using Open3D Web visualizer[[cite:&open3dDocVisualizer;&open3dDocNumPy]]
on [[http://localhost:8888]].

#+begin_src python
import numpy as np
import open3d as o3d
import pye57

if  __name__=='__main__':
    print("Loading Brotfabrik.e57 with pye57 and Open3D")

    e57 = pye57.E57("b:\PointClouds\Data\Brotfabrik.e57")
    data = e57.read_scan(
                0,
                intensity=True,
                colors=True,
                ignore_missing_fields=True)

    x = np.array(data["cartesianX"])
    y = np.array(data["cartesianY"])
    z = np.array(data["cartesianZ"])

    i = np.array(data["intensity"])
    r = np.array(data["colorRed"])
    g = np.array(data["colorGreen"])
    b = np.array(data["colorBlue"])

    r = np.divide(r, 256.0); # r = np.multiply(r, i)
    g = np.divide(g, 256.0); # g = np.multiply(g, i)
    b = np.divide(b, 256.0); # b = np.multiply(b, i)

    xyz = np.zeros((np.size(x), 3))
    rgb = np.zeros((np.size(r), 3))

    xyz[:, 0] = np.reshape(x, -1)
    xyz[:, 1] = np.reshape(y, -1)
    xyz[:, 2] = np.reshape(z, -1)

    rgb[:, 0] = np.reshape(r, -1)
    rgb[:, 1] = np.reshape(g, -1)
    rgb[:, 2] = np.reshape(b, -1)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(xyz)
    pcd.colors = o3d.utility.Vector3dVector(rgb)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries(
        [pcd],
        point_show_normal=False)
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Open3DpyE57.png}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Open3DpyE571.png}
}
\caption{Brotfabrike.57 loaded with Open3D and PyE57.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: Brotfabrike.57 loaded with Open3D and PyE57.
 [[./Open3DpyE57.png]]
#+attr_html: :width 500px
#+CAPTION: Brotfabrike.57 loaded with Open3D and PyE57.
[[./Open3DpyE571.png]]
#+END_center
#+END_EXPORT

* Point Cloud Data Sets                                             :publish:

In order to get familiar with /point cloud/ processing technologies it is important
to get access to representative /point cloud data sets/. The following is a collection
of resources for such data.

** Redwool Data Sets

[[http://www.redwood-data.org/][Redwood-data-org]][[cite:&redwookDataOrg]] provides high quality /point cloud/ data from
indoor scene scans as well as for object scans.

*** Indoor Scans

The [[http://redwood-data.org/indoor_lidar_rgbd/download.html][data sets]] are provided in association to the /Colored Point Cloud Registration Revisited/
publication[[cite:&Park2017]]. They provide ground truth high quality /laser scans/ and /RGB-D/ data sets
of five different indoor scenes. This allows the direct comparsion of reconstruction results to the
real scenery. In addition is their processing pipeline completely implmented in [[http://www.open3d.org/docs/release/tutorial/pipelines/colored_pointcloud_registration.html][Open3D]].

In the following we will come back to these data sets and use them intensively.

The entire dataset, including both RGB-D scans and reconstructed models, is in the public domain.
Any part of the dataset can be used for any purpose with proper attribution. If you use any of the
data, please cite our paper below.

#+begin_example
@inproceedings{Park2017,
	author    = {Jaesik Park and Qian-Yi Zhou and Vladlen Koltun},
	title     = {Colored Point Cloud Registration Revisited},
	booktitle = {ICCV},
	year      = {2017},
}
#+end_example

*** Object Scans

Additionally, a large data base of more than ten thousand 3D scans of real objects is provided
by the [[http://redwood-data.org/3dscan/][red-wood-org]] group[[cite:&Choi2016]]. These scans can be easily accessed by Open3D.

#+begin_src python
import redwood_3dscan as rws
import open3d as o3d

if  __name__=='__main__':
    print("Read Redwood Object Scans")

    rws.download_mesh("00033")
    mesh = o3d.io.read_triangle_mesh("data/mesh/00033.ply")
    mesh.compute_vertex_normals()
    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries([mesh])
#+end_src

For that to work the files from the corresponding [[https://github.com/isl-org/redwood-3dscan][github repository]] have to be installed into the
very same directory as the example script.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Example from the Large Object Scan data set of redwood-data-org loaded with Open3D.
#+NAME:   fig:redwood-00033
[[./redwood-00033.PNG]]

** ModelNet40 and ModelNet10

The goal of the [[https://modelnet.cs.princeton.edu/][Princeton ModelNet project]][[cite:&wuShapeNets]] is to provide researchers in computer vision,
computer graphics, robotics and cognitive science, with a comprehensive clean collection of 3D CAD models
for objects.

The [[https://modelnet.cs.princeton.edu/][ModelNet40]][[cite:&wuShapeNets;&modelNet40]] dataset is a compilation of 12,308 CAD models of
point clouds of common objects.
It includes 40 object categories and the dataset is divided into 9840 models for training and 2468 models
for validation. All the point clouds are pre-aligned into a canonical frame of reference.
It is, however, a syntetical dataset.

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: 3D ShapeNets: A Deep Representation for Volumetric Shapes: [[http://3dvision.princeton.edu/projects/2014/3DShapeNets/poster.pdf][Poster]]
#+NAME:   fig:3DShapeNetsPoster
[[./3DShapeNets.PNG]]

The datasets are available for [[http://3dvision.princeton.edu/projects/2014/3DShapeNets/][download]][[cite:&3DShapeNets]]. The data is stored in =.off= file format and
sorted into folder bins for the categories, e.g. =bed/train/bed_0179.off=.

All CAD models are downloaded from the Internet and the original authors hold the copyright of the
CAD models. The label of the data was obtained by us via /Amazon Mechanical Turk service/ and it is
provided freely. This dataset is provided for the convenience of academic research only.

The [[https://modelnet.cs.princeton.edu/][ModelNet10]] is just a smaller collection of CAD models of only 10 categories.

** ShapeNet

[[https://shapenet.org][ShapeNet]][[cite:&shapeNet;&shapenet2015]] is an ongoing effort to establish a richly-annotated,
large-scale dataset of 3D shapes. It is a collaborative effort between researchers at Princeton,
Stanford and TTIC.

The /ShapeNet core/ dataset contains 57,448 CAD models of man-made objects in 55 categories. Each
model is sampled to 2048 points with three Cartesian coordinates. The dataset is not fully annotated.

/ShapeNetPart/ is a fully annotated subset of the core dataset with 16,881 CAD model in 16 categories and
it is divided into three section: 12,137 shapes for training, 1870 shapes for validation and 2874 shapes
for testing.

The ShapeNet datasets has the following [[https://shapenet.org/terms][Terms of Use]].

** S3DIS

The [[http://buildingparser.stanford.edu/dataset.html][Standfort 3D Indoor Segmentation (S3DIS)]][[cite:&S3DIS;&7780539;&2017arXiv170201105A]] dataset is a subset of
the /2D-3D-Semantics/ dataset. It is one of the benchmark datasets for point cloud segmentation tasks. It contains
6 indoor areas with 271 rooms. There are 13 categories in total, such as ceiling, floor, wall, door, etc.
Each point has 9 dimentsions including /XYZ/ and /RGB/, and normalized /XYZ/.

The dataset can be requests [[https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&w=1][here]] and a [[https://youtu.be/39z4ukLFrvc?list=TLGG3OX8GHZJmwcyMDA3MjAyMg][Youtube preview]] is also available.

* Point Cloud Processing                                            :publish:

An automatic workflow is important for utilizing poind cloud data.

Classic automation workflow as described by Florent Poux[[cite:&phdthesisPoux]].
A [[https://www.youtube.com/watch?v=zOaHuxMem5M][video presentation]] given at the [[https://ncgeo.nl/index.php/en/actueelgb/nieuwsgb/item/2818-seminar-point-clouds-2020][NCG Point Cloud Seminar June 2020]].
Additional [[https://www.youtube.com/watch?v=ogx6DjzzHWk][presentations]] about voxelization and segmentation was held at
GRID research lab at UNSW. See also [[https://www.youtube.com/watch?v=f8cJAT_A-2Y][this]] video.

https://people.utwente.nl/s.nikoohemat?tab=research


- Data acquisition
- Pre-Processing
- Registration
- Segmentation

  Automation in detecting objects by grouping points that share a similarity
  and decisive criterion is the basis for segmentation, thus classification. This step is
  crucial since accuracy of the subsequent processes depends on the validity of the
  segmentation results and requires to balance flexibility with the ease of use[[cite:&phdthesisPoux]].

  It is important to note that the major challenges concern the domain specialization,
  which will majorly orient a classification approach. Classifiers that learn from
  previous or available knowledge differ from their approach, thus their results. They
  are usually categorized in 3 ensembles as in being /supervised learning/ (from a set of
  features to a labelled data) /unsupervised learning/ (structure detection by pattern
  recognition) and /reinforcement learning/ (functional inference through a set of state
  and actions). While /supervised/ method often proves more efficient than its
  /unsupervised/ analogue, they can suffer from over-fitting which limits their
  extensibility and interoperability with wider applications. This is a main concern for
  massive automation[[cite:&phdthesisPoux]].

- Classifiction
- Structuration
- Application

Point clouds require analysis to obtain information about contained objects and spatial properties.
However, point clouds are challenging to classify automatically.
It’s a question of who does the labelling, how fast they can do it, and how accurate it is.

There are four different ways that LiDAR data classification can occur, and the process of automating
data classification is currently progressing through these multiple stages. Realistically, the best
technology on the market sits somewhere within the third stage of the journey towards automation.

The article [[https://www.mdpi.com/2072-4292/14/3/446][Evaluating the Quality of Semantic Segmented 3D Point Clouds]][[cite:&Barnefske2022]] provides
a good review of the state of affaires on point cloud processing.

** Classification, Object Detection and Segmentation

- /Classification/ is the assignment of a class feature (label) to one object.
  This can be a single point, a point cloud, a segment of a point cloud or another geometry
  type. Usually, semantic labels or IDs are assigned.

- In /object detection/, specific objects are defined based on geometric
  or spectral features in the point clouds. The individual object and not the entire point cloud
  is of interest, so that large parts of the point cloud are not evaluated in detail. Several
  objects in a point cloud can be detected and a unique identifier is obtained. Object detection
  is often used in conjunction with tracking objects in applications with multiple sub-point
  clouds. The objects are usually roughly described in terms of geometric size, position and
  orientation using bounding boxes. In other cases, it is not the objects as a whole that are of
  interest, but only certain surfaces or shapes. These are searched for in the point clouds
  (shape detection).

- The /semantic segmentation/ has the goal of extending the features of the points by semantic
  labels. Semantic labels are semantic classes that usually describe real-world objects. The
  difference for the classification is that the segments are formed in this process step and
  a label is set for all points of the segment. A semantic segment can consist of several
  geometrically independent segments. For example, a point can belong to the class table;
  complementarily, it can belong to the subclass table leg. Moreover, the results of the
  classification of each point can form a new segment.

- An /instance segment/ describes the geometric shape of one object. Instances in a point cloud
  can be distinguished by a unique identifier. An instance is usually enriched with semantic
  information. Points of the same semantic segment describe different objects. For example,
  if two tables are in one point cloud, then both carry the same semantic label. In order
  to distinguish the tables, instances must be created. Each table is an instance, which
  usually consists of a geometrically connected point cloud segment. The creation of a digital
  twin goes beyond this idea. For modeling a digital twin, new parametrized objects have to
  be formed that describe the point cloud content by generalizations such as a simple geometry.

** Stage 1: Fully manual

Traditionally, data classification has been carried out manually - assessing point clouds and assigning
point clusters with classification categories. You are manually working with point clouds and carrying
out tasks repeatedly. No matter how often you mark similar objects like a road, you will always have to
do it again. That means you don't get any chance to automate repeated tasks or generate training data
for AI/machine learning.

** Stage 2: Cross-checked teaching

As we move towards the application of AI/ML, algorithms can deliver a first-pass at classification,
which surveyors then manually check for accuracy. The algorithm removes the need for cross-checks
during processing and limits the need to set too many classification parameters. Just as importantly,
this type of automated classification deliberately uses manual inputs to improve an AL/ML algorithm,
helping teach the software how to upgrade outcomes. However, there are still a lot of manual tasks
within cross-checked processes.

** Stage 3: Automated verification

As classification algorithms improve, not every classification assignment needs cross-check verification.
Just like standard object detection software, probability assignment can be made for each classification,
and parameters set to generate cross-check reviews based on the level of certainty — for example, anything
under 80% accuracy. Your ability to set different levels of certainty enables users to trade speed for
accuracy depending on the nature of the project.

Automated verification systems are flexible and fast. Realistically, the amount of manual cross-checking
required will vary by project specification and the complexity (or novelty) of the data being analysed.
However, like with /cross-checked teaching/, automated verification uses human input to further refine
and train models - helping reduce long-term manual requirements for effective LiDAR data classification.

** Stage 4: Fully automated

The goal of training data classification algorithms is a fully automated system. Realistically, this is
simply an improved version of automated verification that is good enough to confidently assign classifications
without need for verification. However, the quality assurance of confidence assessments (and the possibility
of manual review) would remain in any quality automated LiDAR classification software.

* Deep Learning on Point Clouds                                     :publish:

This section is concerned about /Machine Learning/ techniques for point cloud
processing. A number of /Deep Learning Networks/ are mentioned. A survey about
methods utilizing deep learning on point clouds can be found [[https://arxiv.org/abs/1912.12033][here]][[cite:&guoSurvey2019]].

** Overview, Lectures, Talks and Courses

The following is a unordered list of available online resources to the topic at hand.

- [[https://github.com/timzhang642/3D-Machine-Learning][3D Machine Learning]]:

  In recent years, tremendous amount of progress is being made in the field of 3D Machine Learning,
  which is an interdisciplinary field that fuses computer vision, computer graphics and machine
  learning. The repo is derived from Yuxuan (Tim) Zhang's study notes and it can be used as a place
  for triaging new research papers.

- Youtube video [[https://www.youtube.com/watch?v=gm_oW0bdzHs][Deep Learning on Point Clouds]] by Hao Su:

  This is a talk the focus on state of the art deep learning techniques. Very informative.
  Hao Su is one of the authors of the influential /PointNet/ deep net architecture for point clouds.

- [[https://github.com/maziarraissi][Maziar Raissi]] Github page:

  Professsor Raissi provides a huge collection of Youtube video [[https://www.youtube.com/channel/UCxEiGqJ2e-Mg9oQMjVv6poQ/videos][lectures]] in the field of machine and deep
  learning. At his Github page you can find a lot of background material as well as presentation documents.

** Segmentation Tasks

*** PointNet

[[https://github.com/charlesq34/pointnet][PointNet]][[cite:&qi2016pointnet;&PointNet]] proposed a novel deep net architecture for point clouds as
unordered point sets. The PointNet family of models provides a simple, unified architecture for
applications ranging from /object classification/, /part segmentation/, to /scene semantic parsing/.

- [[https://www.youtube.com/watch?v=Cge-hot0Oc0][Video presentation.]]
- Maziar Raissi about [[https://www.youtube.com/watch?v=hgtvli571_U][PointNet]].
- [[https://www.youtube.com/watch?v=_py5pcMfHoc][PointNet paper explained.]]
- [[https://www.youtube.com/watch?v=GGxpqfTvE8c][Point Cloud Classification with PointNet - Create, Train and Predict in Keras and TensorFlow]].
- [[https://www.youtube.com/watch?v=pD-hdKizumM][Tutorial - Learning on Point Clouds]].

Additional Resources:
- Tensorflow 2
  - [[https://github.com/YoungsonZhao/pointnet-tf2][pointnet-tf2]]
  - [[https://github.com/YuhangJi/Python/tree/master/PointcloudSegmentation/PointNet][PointNet]]
  - [[https://github.com/IsaacGuan/PointNet-Plane-Detection][Point-Net plane detection]]
  - [[https://github.com/luis-gonzales/pointnet_own][Tensorflow2 implementation of PointNet]]
  - [[https://keras.io/examples/vision/pointnet][PointNet with Keras]]
  - [[https://github.com/soumik12345/point-cloud-segmentation][Point Cloud Segmentation by Soumik Rakshit & Sayak Paul]]

**** PointNet Classification with Keras

#+caption: Example: pointnet_classification.py
#+begin_src python
import os
import glob
import trimesh
import numpy as np
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from matplotlib import pyplot as plt

def get_data(name):
    origin = '/'.join(
            [
                'http://3dvision.princeton.edu',
                'projects/2014/3DShapeNets',
                name
            ]
        )

    cache_subdir = 'datasets'

    data_dir = os.path.join(
                r'C:\Users\Bru\.keras',
                cache_subdir,
                name)

    if not os.path.exists(data_dir):
        ret_dir = tf.keras.utils.get_file(
            fname          = "modelnet.zip",
            origin         = origin,
            untar          = False,
            md5_hash       = None,
            file_hash      = None,
            cache_subdir   = cache_subdir,
            hash_algorithm = 'auto',
            extract        = True,
            archive_format = 'auto',
            cache_dir      = None
        )

        ret_dir = os.path.join(
            os.path.dirname(ret_dir), name)

        assert(ret_dir == data_dir)

    print("Data directory = ", data_dir)

    return data_dir

def show_example(data_dir, subpath):
    mesh = trimesh.load(
            os.path.join(data_dir, subpath))
    mesh.show()

    points = mesh.sample(2048)

    fig = plt.figure(figsize=(5, 5))
    ax = fig.add_subplot(111, projection="3d")
    ax.scatter(points[:, 0], points[:, 1], points[:, 2])
    ax.set_axis_off()
    plt.show()

def parse_dataset(data_dir, num_points=2048):

    train_points = []
    train_labels = []
    test_points  = []
    test_labels  = []
    class_map    = {}

    folders = glob.glob(os.path.join(data_dir, "[!README]*"))

    print (folders)

    for i, folder in enumerate(folders):
        print("processing class: {}".
                format(os.path.basename(folder)))

        # store folder name with ID so we can retrieve later
        class_map[i] = os.path.basename(folder)

        # gather all files
        train_files = glob.glob(os.path.join(folder, "train/*"))
        test_files  = glob.glob(os.path.join(folder, "test/*"))

        # each train and test file is loaded, then sampled and
        # finally labeled.

        for f in train_files:
            train_points.append(trimesh.load(f).sample(num_points))
            train_labels.append(i)

        for f in test_files:
            test_points.append(trimesh.load(f).sample(num_points))
            test_labels.append(i)

    #  {0: 'bathtub', 1: 'bed', 2: 'chair', ...}
    print ("class_map = ", class_map)

    return (
        np.array(train_points),
        np.array(test_points ),
        np.array(train_labels),
        np.array(test_labels ),
        class_map,
    )

def print_dataset(dataset):
    cnt = 0
    for element in train_dataset:
        print (element)
        # (<tf.Tensor: shape=(2048, 3), dtype=float64, numpy=
        #   array([[ 12.497,  -6.585, -15.246],
        #          [ 14.498,  -6.073,  13.234],
        #          [ 16.975,  -6.368,  14.211],
        #          ...,
        #          [ 17.316, -28.938,  14.211],
        #          [ -7.109, -26.313,   6.307],
        #          [-13.576,  -6.359,  -3.928]])>,
        #  <tf.Tensor: shape=(), dtype=int32, numpy=0>
        # )
        if cnt > 2: break
        cnt += 1

def augment(points, label):
    # jitter points
    points += tf.random.uniform(
                    points.shape,
                    -0.005, 0.005,
                    dtype=tf.float64)
    # shuffle points
    points = tf.random.shuffle(points)

    return points, label

def conv_bn(x, num_filters):
    x = layers.Conv1D(
        num_filters,
        kernel_size = 1,
        padding     = "valid",
        # strides              = 1,
        # data_format          = "channels_last",
        # dilation_rate        = 1,
        # groups               = 1,
        # activation           = None,
        # use_bias             = True,
        # kernel_initializer   = "glorot_uniform",
        # bias_initializer     = "zeros",
        # kernel_regularizer   = None,
        # bias_regularizer     = None,
        # activity_regularizer = None,
        # kernel_constraint    = None,
        # bias_constraint      = None
    )(x)

    x = layers.BatchNormalization(
        momentum = 0.0,   # default = 0.99,
        # axis                        = -1,
        # epsilon                     = 0.001,
        # center                      = True,
        # scale                       = True,
        # beta_initializer            = "zeros",
        # gamma_initializer           = "ones",
        # moving_mean_initializer     = "zeros",
        # moving_variance_initializer = "ones",
        # beta_regularizer            = None,
        # gamma_regularizer           = None,
        # beta_constraint             = None,
        # gamma_constraint            = None
    )(x)

    return layers.Activation("relu")(x)

def dense_bn(x, units):
    # regular densely-connected NN layer
    #   output = activation(dot(input, kernel) + bias)
    x = layers.Dense(
        units,
        # activation           = None,
        # use_bias             = True,
        # kernel_initializer   = "glorot_uniform",
        # bias_initializer     = "zeros",
        # kernel_regularizer   = None,
        # bias_regularizer     = None,
        # activity_regularizer = None,
        # kernel_constraint    = None,
        # bias_constraint      = None
    )(x)

    x = layers.BatchNormalization(
        momentum = 0.0,   # default = 0.99,
        # axis                        = -1,
        # epsilon                     = 0.001,
        # center                      = True,
        # scale                       = True,
        # beta_initializer            = "zeros",
        # gamma_initializer           = "ones",
        # moving_mean_initializer     = "zeros",
        # moving_variance_initializer = "ones",
        # beta_regularizer            = None,
        # gamma_regularizer           = None,
        # beta_constraint             = None,
        # gamma_constraint            = None
     )(x)

    return layers.Activation("relu")(x)

#
# Regularizers allow you to apply penalties on layer
# parameters or layer activity during optimization.
# These penalties are summed into the loss function
# that the network optimizes.
#

class OrthogonalRegularizer(
        keras.regularizers.Regularizer):
    def __init__(
            self,
            num_features,
            l2reg = 0.001):
        self.num_features = num_features
        self.l2reg        = l2reg
        self.eye          = tf.eye(
                    num_rows    = num_features,
                    # num_columns = None,
                    # batch_shape = None,
                    # dtype       = tf.dtypes.float32,
                    # name        = None
                ) # Construct an identity matrix,
                  # or a batch of matrices.

        #print (self.eye)
        # tf.Tensor(
        # [[1. 0. 0.]
        #  [0. 1. 0.]
        #  [0. 0. 1.]], shape=(3, 3), dtype=float32)

    def __call__(self, x):
        #print('x = ', x)
        # x = Tensor(
        #       "dense_2/BiasAdd:0",
        #       shape=(None, 9),
        #       dtype=float32)

        # reshape example
        # t1 = [[1, 2, 3],
        #       [4, 5, 6]]
        # print(tf.shape(t1).numpy()) # [2,3]
        # t2 = tf.reshape(t1,[6]) # <tf.Tensor:
                                  #   shape=(6,), dtype=int32,
                                  #   numpy=array([1,2,3,4,5,6],
                                  #               dtype=int32)>
        # tf.reshape(t2, [3, 2])  # <tf.Tensor:
                                  #   shape=(3, 2), dtype=int32,
                                  #   numpy=array([[1,2],
                                  #                [3,4],
                                  #                [5,6]],
                                  #               dtype=int32)>
        # If one component of shape is the special value -1,
        # the size of that dimension is computed so that the
        # total size remains constant. In particular, a shape
        # of [-1] flattens into 1-D. At most one component of
        # shape can be -1.
        x   = tf.reshape(
                    x,
                    (
                        -1,
                        self.num_features,
                        self.num_features
                    )
                )
        #print('x = ', x)
        # x = Tensor(
        #       "dense_2/ActivityRegularizer/Reshape:0",
        #       shape=(None, 3, 3),
        #       dtype=float32)

        # tensordot sums the product of elements from a and b
        # over the indices specified by axes.
        # Example 1: When a and b are matrices (order 2),
        #       the case axes = 1 is equivalent to matrix
        #       multiplication.
        # Example 2: When a and b are matrices (order 2),
        #       the case axes = [[1], [0]] is equivalent
        #       to matrix multiplication.
        # Example 3: When a and b are matrices (order 2),
        #       the case axes = 0 gives the outer product,
        #       a tensor of order 4.
        # Generally: order(c)=order(a)+order(b)-2*len(axes[0])

        xxt = tf.tensordot(x, x, axes=(2, 2))
        #print('xxt = ', xxt)
        # xxt = Tensor(
        #         "dense_2/ActivityRegularizer/Tensordot:0",
        #         shape=(None, 3, None, 3),
        #         dtype=float32)

        xxt = tf.reshape(
                    xxt,
                    (
                        -1,
                        self.num_features,
                        self.num_features
                    )
                )
        #print('xxt = ', xxt)
        # xxt = Tensor(
        #         "dense_2/ActivityRegularizer/Reshape_1:0",
        #         shape=(None, 3, 3),
        #         dtype=float32)

        # Computes the sum of elements across dimensions of
        # a tensor.
        result = tf.reduce_sum(
            input_tensor = (
                self.l2reg * tf.square(xxt - self.eye)),
            axis         = None,
            keepdims     = False,
            name         = None
        )
        #print('result = ', result)
        # result = Tensor(
        #           "dense_2/ActivityRegularizer/Sum:0",
        #         shape=(),
        #         dtype=float32)

        return result


def tnet(inputs, num_features):
    # Initalise bias as the indentity matrix
    bias = keras.initializers.Constant(
                    np.eye(num_features).flatten())

    regularizer = OrthogonalRegularizer(num_features)

    x = conv_bn(inputs, num_filters = 32)
    x = conv_bn(x,      num_filters = 64)
    x = conv_bn(x,      num_filters = 512)

    x = layers.GlobalMaxPooling1D(
            data_format="channels_last", keepdims=False
        )(x)

    x = dense_bn(x, units = 256)
    x = dense_bn(x, units = 128)

    x = layers.Dense(
        units = num_features * num_features,
        kernel_initializer   = "zeros",
        bias_initializer     = bias,
        activity_regularizer = regularizer,
        # default arguments
        # activation           = None,
        # use_bias             = True,
        # kernel_regularizer   = None,
        # bias_regularizer     = None,
        # kernel_constraint    = None,
        # bias_constraint      = None
    )(x)

    feat_T = layers.Reshape((num_features, num_features))(x)

    # Apply affine transformation to input features
    result = layers.Dot(axes=(2, 1))([inputs, feat_T])

    return result

def point_net(inputs, num_classes):
    x = tnet(inputs, num_features = 3)

    x = conv_bn(x, num_filters = 32)
    x = conv_bn(x, num_filters = 32)

    x = tnet(x, num_features = 32)

    x = conv_bn(x, num_filters = 32)
    x = conv_bn(x, num_filters = 64)
    x = conv_bn(x, num_filters = 512)

    x = layers.GlobalMaxPooling1D()(x)

    x = dense_bn(x, units = 256)
    x = layers.Dropout(0.3)(x)

    x = dense_bn(x, units = 128)
    x = layers.Dropout(0.3)(x)

    outputs = layers.Dense(
                num_classes,
                activation = "softmax")(x)

    model = keras.Model(inputs  = inputs,
                        outputs = outputs,
                        name    = "pointnet")

    return model

def train_model(
        model, train_dataset, validation_dataset):
    model.summary()

    model.compile(
        loss      = "sparse_categorical_crossentropy",
        optimizer = keras.optimizers.\
                        Adam(learning_rate = 0.001),
        metrics   = ["sparse_categorical_accuracy"],
    )

    model.fit(train_dataset,
              epochs          = 20,
              validation_data = validation_dataset)

def test_model(
        model, test_dataset, class_map):
    data = test_dataset.take(1)

    points, labels = list(data)[0]
    points = points[:8, ...]
    labels = labels[:8, ...]

    # run test data through model
    preds = model.predict(points)
    preds = tf.math.argmax(preds, -1)

    points = points.numpy()

    # plot points with predicted class and label
    fig = plt.figure(figsize = (15, 10))
    for i in range(8):
        ax = fig.add_subplot(2, 4, i + 1, projection="3d")
        ax.scatter(
                points[i, :, 0],
                points[i, :, 1],
                points[i, :, 2])
        ax.set_title(
            "pred: {:}, label: {:}".format(
                class_map[preds[i].numpy()],
                class_map[labels.numpy()[i]]
            )
        )
        ax.set_axis_off()
    plt.show()


if  __name__=='__main__':
    tf.random.set_seed(1234)

    data_dir = get_data('ModelNet10')

    show_example(
        data_dir, "chair/train/chair_0001.off")

    NUM_POINTS  = 2048
    NUM_CLASSES = 10
    BATCH_SIZE  = 32

    ( train_points, test_points,
      train_labels, test_labels,
      CLASS_MAP ) = parse_dataset(data_dir, NUM_POINTS)

    # The given tensors are sliced along their first
    # dimension. This operation preserves the structure
    # of the input tensors, removing the first dimension
    # of each tensor and using it as the dataset
    # dimension. All input tensors must have the same
    # size in their first dimensions.

    train_dataset = tf.data.Dataset.\
        from_tensor_slices((train_points, train_labels))
    test_dataset  = tf.data.Dataset.\
        from_tensor_slices(( test_points,  test_labels))

    print_dataset(train_dataset)

    # 1) shuffle returns dataset
    # 2) map applies map_func, i.e. augment to each element
    #    of this dataset, and returns a new dataset containing
    #    the transformed elements, in the same order as they
    #    appeared in the input.
    # 3) batch combines consecutive elements of this dataset
    #    into batches.
    #      dataset = tf.data.Dataset.range(8) # [0,1,2,...,7]
    #      dataset = dataset.batch(3)
    #      list(dataset.as_numpy_iterator())
    #      # [array([0,1,2]), array([3,4,5]), array([6,7])]

    train_dataset = train_dataset.shuffle(
                        len(train_points)
                    ).map(augment
                    ).batch(BATCH_SIZE)

    test_dataset  =  test_dataset.shuffle(
                        len( test_points)
                    ).batch(BATCH_SIZE)

    validation_dataset = test_dataset

    inputs = keras.Input(shape=(NUM_POINTS, 3))

    model = point_net(inputs, NUM_CLASSES)

    train_model(model, train_dataset, validation_dataset)
    test_model (model, test_dataset, CLASS_MAP)
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][CAD model]{
  \includegraphics[width=.3\linewidth]{./pointnet_classification-1.png}
}
\qquad
\subfloat[][Sampled model]{
  \includegraphics[width=.3\linewidth]{./pointnet_classification-2.png}
}
\qquad
\subfloat[][Classification]{
  \includegraphics[width=.8\linewidth]{./pointnet_classification-3.png}
}
\caption{PointNet classification of ModelNet10 dataset..}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: PointNet classification of ModelNet10 dataset.: CAD model
#+name:   fig:pointnet_classification-1
[[./pointnet_classification-1.png]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: PointNet classification of ModelNet10 dataset.: Sampled model
#+name:   fig:pointnet_classification-2
[[./pointnet_classification-2.png]]

#+attr_html: :width 800px
#+attr_latex: :width 120px
#+caption: PointNet classification of ModelNet10 dataset.: Classification
#+name:   fig:pointnet_classification-3
[[./pointnet_classification-3.png]]
#+end_export

**** PointNet Segmentation with Keras

*** PointNet++

[[https://github.com/charlesq34/pointnet2][PointNet++]][[cite:&PointNet2;&PointNet2Paper]] enhances the [[https://github.com/charlesq34/pointnet][PointNet]][[cite:&qi2016pointnet;&PointNet]] network
for recognizing finer grained details and working on more complex scenes.

- Maziar Raissi about [[https://www.youtube.com/watch?v=FAqN0KK_2kg][PointNet++]].

*** Dynamic Graph CNN

[[https://github.com/WangYueFt/dgcnn][Dynamic Graph CNN]][[cite:&DynamicGraphCNNPaper;&DynamicGraphCNN]]
proposes a new neural network module dubbed /EdgeConv/ suitable for /CNN-based/ high-level
tasks on point clouds including classification and segmentation.

- [[https://www.youtube.com/watch?v=bBS1xLzqp1U][CSC2547 Dynamic Graph CNN for Learning on Point Cloud]].
- Maziar Raissi about [[https://www.youtube.com/watch?v=yrYrwB6mQ4M][Dynamic Graph CNN]].
- [[https://www.youtube.com/watch?v=iUUt6EOvs0A][Paper summary.]]
- [[https://www.youtube.com/watch?v=CyFANS_Itb8][PR-295: Dynamic Graph CNN for Learning on Point Clouds]].
- [[https://www.youtube.com/watch?v=NXZLmTzRGdw][Graph Neural Networks on Point Clouds]].
- [[https://www.youtube.com/watch?v=zCEYiCxrL_0][An Introduction to Graph Neural Networks: Models and Applications]].
- [[https://www.youtube.com/watch?v=ialwaQyKv5M][CSC:2547 | Winter 2021 | Dynamic Graph CNN for Learning on Point Cloud]]

Additional Resources:
- [[https://github.com/YuhangJi/CASem][Modified DGCNN for Archtectural scene]]

*** PointCNN

[[https://github.com/yangyanli/PointCNN][PointCNN]][[cite:&PointCNNPaper]]
is a simple and general framework for feature learning from point clouds. The key to the success of
CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data
represented densely in grids.

- [[https://www.youtube.com/watch?v=Sor2T17Qn20][PointCNN-based Individual Tree Detection Using LiDAR Point Clouds]]

*** PointSIFT

[[https://github.com/MVIG-SJTU/pointSIFT][PointSIFT]][[cite:&jiang2018pointsift]]
is a semantic segmentation framework for 3D point clouds. It is based on a simple module which
extract featrues from neighbor points in eight directions.

*** Point Transformer

[[https://github.com/POSTECH-CVLab/point-transformer][Point Transformer]][[cite:&zhao2021point]]
design self-attention layers for point clouds and use these to construct
self-attention networks for tasks such as semantic scene segmentation, object part segmentation,
and object classification. Our Point Transformer design improves upon prior work across domains and
tasks.

A dedicated [[https://github.com/lucidrains/point-transformer-pytorch][PyTorch implementation]] is also available.

- [[https://www.youtube.com/watch?v=UAijTLXkupQ][Spatial Transformer for 3D Point Clouds]]
- [[https://www.youtube.com/watch?v=YT_l8ZGEYyg][Point Transformer]]

*** RandLA-Net

[[https://github.com/QingyongHu/RandLA-Net][RandLA-Net]][[cite:&hu2020randlanet;&randla-net]] is a simple and efficient neural architecture for semantic
segmentation of large-scale 3D point clouds.

- [[https://www.youtube.com/watch?v=Ar3eY_lwzMk][Video Presentation]]
- [[https://www.youtube.com/watch?v=YD7tjs2M_0g][RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds]].
- [[https://www.youtube.com/watch?v=oU9VNTXwfqw][RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds]]

*** Mix3D

[[https://github.com/kumuji/mix3d][Mix3D]][[cite:&Nekrasov213DV]] is a data augmentation technique for segmenting large-scale 3D scenes.

While /PointNet/ and similar approaches can classify individual objects
or segment larger scenes by splitting them into cubical
(or spherical) chunks, recent high-capacity models such as
/MinkowskiNets/  and /SparseConvNets/ have large receptive fields and can
directly process full rooms or even outdoor scenes without splitting them up.
By doing so, they can capture the global context of a scene. Context refers to
the strong configuration rules present in man-made environments[[cite:&Nekrasov213DV]].

** Amazon SageMaker

[[https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html][Amazon SageMaker]] is a fully managed machine learning service. With SageMaker,
data scientists and developers can quickly and easily build and train machine
learning models, and then directly deploy them into a production-ready hosted
environment. It provides an integrated /Jupyter/ authoring notebook instance
for easy access to your data sources for exploration and analysis, so you don't
have to manage servers. It also provides common machine learning algorithms
that are optimized to run efficiently against extremely large data in a distributed
environment. Deploy a model into a secure and scalable environment by launching it
with a few clicks from /SageMaker Studio/ or the /SageMaker console/.
Training and hosting are billed by minutes of usage, with no minimum fees and no
upfront commitments.

*** Access AWS SageMager's Endpoint with Python

#+begin_src python
import boto3
import json
import numpy as np

client = boto3.client('sagemager-runtime')

f = open('./test_1.jpg', 'rb')
data = f.read()

response = client.invoke_endpoint(
    EndpointName='lbs-tree-hotdog-tea-1',
    Body=data
)
print(response)

predictions = json.loads(response['Body'].read().decode())
print(predictions)

categrories = [ 'tree', 'hotdog', 'tea' ]

prediction = np.argmax(predictions)
result = categories[prediction]
print(result)
#+end_src

*** Access AWS SageMager's Endpoint with the Cloud Shell

#+begin_src shell
aws s3 ls

ls -l

aws sagemaker-runtime invoke-endpoint --endpoint-name lbs-tree-hotdog-tea-1 --body fileb://./test-2.jpg output.txt

ls -l
cat ./output.txt
#+end_src

*** Tipps for using AWS

- AWS Organizations

  It helps you centrally manage and govern your environment as you grow and scale your
  AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts
  and allocate resources, group accounts to organize your workflow, apply policies to
  accounts and groups for governance, and simplify billing by using a single payment method
  for all accounts.

  Additionally, AWS Organizations is integrated with other AWS services so you can define
  central configurations, security mechanisms, audit requirements, and resource sharing
  across accounts in your organization. It is available at no additional charge.

  It is basically a collection of multiple AWS accounts.

  Use separate accounts for each project you are doing. Keep things separated.

- /Multi-Account Containers/ Firefox plugin

  It is a Firefox add-on that lets you separate your work, shopping, or personal
  browsing without having to clear history, log in and out, or use multiple
  browsers. It is available to advanced users by setting certain preferences.

- AWS Extend Switch Roles Firefox/Chrome/Edge plugin

  Extend your AWS IAM switching roles. You can set the configuration by aws
  config format.

  Use this plugin to switch role into another account after configuration of this
  plugin.

- AWS Single Sign On (SSO)

  It is where you create, or connect, your workforce indentities in AWS once and
  manage access centrally across your AWS organization. You can choose to manage
  access just to your AWS accounts or cloud applications. You can create user
  identities directly in AWS SSO, or you can bring them from your Microsoft
  Active Directory or a standards-based identity provider, such as Okta
  Universal Directory or Azure AD. With AWS SSO, you get a unified administration
  experience to define, customize, and assign fine-grained access. Your workforce
  users get a user portal to access all their assigned AWS accounts or cloud
  applications. AWS SSO can be flexibly configured to run alongside or replace
  AWS account access management via AWS IAM.

  You can also use the AWS built-in free identity provider.

  #+begin_src shell
aws sso login --profile events

# then enter the code you get from the authorization request page.

aws s3 ls --profile events
  #+end_src

- AWS Cloud Shell

  It is a browser-based shell that gives you command-line access to your
  AWS resources in the selected AWS region. It comes pre-installed with
  popular tools for resource management and creation. You have the same
  credentials as you used to log in to the console.

*** SageMaker Script Mode

The following shows how models can be trained, saved, loaded and run. Then we will
see how the same thing can be achieved using the SageMaker SDK, and SageMaker managed
infrastructure.

- Create a dataset and save

  #+begin_src python
from sklearn import datasets
import pickle

X,y = datasets.make_regression(
                   100,
                   1,
                   noise=5,
                   bias = 0)

pickle.dump(X,y), open('./train.pickle', 'wb')
  #+end_src

- Create a model from the dataset

  #+begin_src python
from sklearn.linear_model import LinearRegression
import pickle

(X, y) = pickle.load(open('./train.pickle', 'rb'))

model = LinearRegression()

model.fit(X, y)
  #+end_src

- Make a test prediction

  #+begin_src python
model.predict([[0], [1], [2], [3]])
  #+end_src

- Save the model file

  #+begin_src python
p = pickle.dumps(model)

pickle.dump(model, open('./model.pickle', 'wb'))
  #+end_src

- Later load the model from a file

  #+begin_src python
from sklearn.linear_model import LinearRegression
import pickle

loaded_model = pickle.load(open('./model.pickle', 'rb'))
  #+end_src

- Make a test prediction on loaded model

  #+begin_src python
loaded_model.predict([[0], [1], [2], [3]])
  #+end_src

- SageMaker Training

  Now, we want to perform at production scale on SageMaker.

  It starts by setting up the SageMaker session. SageMaker abstracts
  away the details of how to interact with the AWS infrastructure.

  The SageMaker SDK has a couple of high-level objects and the
  /Estimator/ is one of these.

  #+begin_src python
import sagemaker
from  sagemaker.sklearn.estimator import SKLearn
import boto3
import os

role    = sagemaker.get_execution_role()
session = sagemaker.Session()
bucket  = session.default_bucket()

s3_prefix = "script-mode-workflow"

pickle_s3_prefix    = f"{s3_prefix}/pickle"
pickle_s3_uri       = f"s3://{bucket}/{pickle_s3_prefix}"
pickle_train_s3_uri = f"{pickle_s3_uri}/train"

train_dir = os.path.join(os.getcwd(), "")

# Update the training data to S3, so it is available
# for SageMaker training.

s3_resource_bucket = boto3.Session().\
    resource("s3").Bucket(bucket)
s3_resource_bucket.Object(os.path.join(
                              pickle_s3_prefix,
                              "train.pickle")`).\
    upload_file(train_dir + "/train.pickle")

# Create some hyperparameters
hyperparameters = {
    'Copy_X': True,
    'fit_intercept': True,
    'normalize': False
}

# More configuration for the model

train_instance_type = "ml.m5.large"

inputs = {
    'train': pickle_train_s3_uri
}

estimator_parameters = {
    'entry_point':'script.py',
    'source_dir': 'script',
    'framework_version': '0.23-1',
    'py_version': 'py3',
    'instance_type': train_instance_type,
    'instance_count': 1,
    'hyperparameters': hyperparameters,
    'role': role,
    'base_job_name': 'Linearregression-model'
}

estimator = SKLearn(**estimator_parameters)

estimater.fit(inputs)
  #+end_src

  When we call /fit/ SageMaker will spin up managed containers, transfer the code and
  data to the container and then start the training. All this happens off of the notebook
  server. We can watch the training through the console, and watch the logs in the /CloudWatch/ Logs.

- SageMaker Endpoint

  We create a /predictor/ by deploying the estimator. Then we can use it
  to make new predictions.

  Remark: Make sure the the /endpoint_name/ used is not currently running!

  #+begin_src python
sklearn_predictor = estimator.deploy(
    initial_instance_count = 1,
    instance_type = 'ml.m5.large',
    endpoint_name = 'linearregression-endpoint'
)

sklearn_predictor.predict([[0], [1], [2], [3]])
  #+end_src

- Clean up

  Running the following will remove the endpoint and configuration.

  #+begin_src python
sklearn_predictor.delete_endpoint(True)
  #+end_src

- The used example script

  #+begin_src python
import argparse
import os
from sklearn.linear_model import LinearRegression
import pickle

def parse_args():
    """
    Parse arguments.
    """
    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as
    # command-line arguments to the script.
    # The example don't use these, but as a template for
    # future use, they are left here.
    parser.add_argument('--copy_X',
                        type=bool, default=True)
    parser.add_argument('--fit_intercept',
                        type=bool, default=True)
    parser.add_argument('--normalize',
                        type=bool, default=False)

    # data directories
    parser.add_argument('--train',
                        type=str,
                        default=os.environ.get(
                            'SM_CHANNEL_TRAIN'))
    parser.add_argument('--test',
                        type=str,
                        default=os.environ.get(
                            'SM_CHANNEL_TEST'))

    # model directory
    parser.add_argument('--model_dir',
                        type=str,
                        default=os.environ.get(
                            'SM_MODEL_DIR'))

    return parser.parser_known_args()

def load_dataset(path):
    """
    Load entire dataset.
    """
    # Find all files with a pickle ext but we only
    # lead the first one in this sample.
    files = [
        os.path.join(path, file)
            for file in os.listdir(path)
                if file.endswith('pickle')
    ]

    if len(files) == 0:
        raise ValueError(
            f"Invalid # of files in dir {path}")

    [X, y] = pickle.load(open(files[0], 'rb'))

    return X, y

def start(args):
    """
    Train a Linear Regression.
    """
    print("Training Mode")

    try:
        X_train, y_train = load_dataset(args.train)
        # X_test, y_test = load_dataset(args.test)

        hyperparameters = {
            'Copy_X': args.copy_X,
            'fit_intercept': args.fit_intercept,
            'normalize': args.normalize
        }

        print("Training...")

        model = LinearRegression()
        model.set_params(**hyperparameters)
        model.fit(X_train, y_train)

        pickle.dump(model, open(
            os.path.join(
                args.model_dir,
                'model.pickle'
            ),
            'wb'))
        except Exception as e:
            # Write an error file. This will be returned
            # as the failureReason in the
            # DescribeTrainingJob result.
            trc = traceback.format.exc()
            with open(
                    os.path.join(output_path, "failure"),
                    'w'
            ) as s:
                s.write("Exception during training: " +
                        str(e) + r'\n' + trc)

            # Printing this causes the exception to be in
            # the training job logs, as well.
            print("Exception during training: " +
                        str(e) + r'\n' + trc,
                  file=sys.stderr)

            # A non-zero exit code causes the training job
            # to be marked as Failed.
            sys.exit(255)

def model_fn(model_dir):
    """
    Load the model for inference.
    """
    loaded_model = pickle.load(
        open(model_dir + '/model.pickle', 'rb'))

    return loaded_model

def predict_fn(input_data, model):
    """
    Apply model to the incoming request.
    """
    return model.predict(input_data)

if __name__ == "__main__":
    args, _ = parse_args()

    start(args)
  #+end_src

* Tooling                                                           :publish:

In this section tools are listed that are useful in the realm of
surface reconstruction.

- [[https://www.meshlab.net/#description][MeshLab]]

  MeshLab is an open source system for processing and editing 3D triangular meshes.
  It provides a set of tools for editing, cleaning, healing, inspecting, rendering,
  texturing and converting meshes. It offers features for processing raw data produced
  by 3D digitization tools/devices and for preparing models for 3D printing.

- [[https://github.com/cnr-isti-vclab/PyMeshLab][PyMeshLab]]

  PyMeshLab is a Python library that interfaces to [[https://www.meshlab.net/#description][MeshLab]].

  Documentation can be found [[https://pymeshlab.readthedocs.io/en/latest/][here]].

  #+caption: PyMeshLab example.
  #+begin_src shell
pip install pymeshlab
  #+end_src

  #+begin_src python
import pymeshlab

ms = pymeshlab.MeshSet()

ms.load_new_mesh('airplane.obj')
ms.generate_convex_hull()
ms.save_current_mesh('convex_hull.ply')

ms.create_noisy_isosurface(resolution=128)

pymeshlab.search('poisson')

out_dict = ms.get_geometric_measures()
print(out_dict['surface_area'])

ms.apply_coord_laplacian_smoothing(stepsmoothnum=10)
ms.save_filter_script('my_script.mlx')

ms = pymeshlab.MeshSet()
ms.load_new_mesh('another_input.obj')
ms.load_filter_script('my_script.mlx')
ms.apply_filter_script()
ms.save_current_mesh('result.obj')
  #+end_src

- [[https://www.meshmixer.com][MeshMixer]]

  Autodesk MeshMixer is a free state-of-the-art software for working with triangle meshes.
  It is discontinued and followed up by Autodesk's comercial Fusion 360. However, it
  provides the following features for free:

  - Drag-and-Drop Mesh Mixing
  - 3D Sculpting and Surface Stamping
  - Robust Convert-to-Solid for 3D printing
  - 3D Patterns & Lattices
  - Hollowing (with escape holes!)
  - Branching Support Structures for 3D printing
  - Automatic Print Bed Orientation Optimization, Layout & Packing
  - Advanced selection tools including brushing, surface-lasso, and constraints
  - Remeshing and Mesh Simplification/Reducing
  - Mesh Smoothing and Free-Form Deformations
  - Hole Filling, Bridging, Boundary Zippering, and Auto-Repair
  - Plane Cuts, Mirroring, and Booleans
  - Extrusions, Offset Surfaces, and Project-to-Target-Surface
  - Interior Tubes & Channels
  - Precise 3D Positioning with Pivots
  - Automatic Alignment of Surfaces
  - 3D Measurements
  - Stability & Thickness Analysis

  The following [[https://www.youtube.com/watch?v=8PWdp5VzKSg&list=PL4PNWcw4F8TLVOpKre38rKBVpRVa4g24A&index=2][Youtube video]] shows usage of MeshMixer.

- [[https://www.blender.org/][Blender]]

  Blenders mission is to get the world’s best 3D CG technology in the hands
  of artists as free/open source software.

  Blender is the free and open source 3D creation suite. It supports the entirety of the
  3D pipeline—modeling, rigging, animation, simulation, rendering, compositing and motion
  tracking, even video editing and game creation. Advanced users employ Blender’s API for
  Python scripting to customize the application and write specialized tools.

  The following [[https://www.youtube.com/watch?v=IN1nyU_CL7A][Youtube video]] shows the usage of Blender for LiDAR scanning postprocessing.
  Additional introductory tutorial material can be found in the Youtube channel of the [[https://www.youtube.com/c/BlenderGuruOfficial][Blender Guru]].

- [[https://github.com/pyvista/pyvista][pyVista]]

  PyVista is...

  -  Pythonic VTK: a high-level API to the Visualization Toolkit (VTK)
  -  mesh data structures and filtering methods for spatial datasets
  -  3D plotting made simple and built for large/complex data geometries

  PyVista is a helper module for the [[https://vtk.org/][Visualization Toolkit (VTK)]] that wraps the
  VTK library through NumPy and direct array access through a variety of methods
  and classes.

  This package provides a Pythonic, well-documented interface exposing VTK's powerful
  visualization backend to facilitate rapid prototyping, analysis, and visual
  integration of spatially referenced datasets.

  #+begin_src shell
pip install pyvista
  #+end_src

  #+caption: Example: pyVistaExample.py
  #+begin_src python
import numpy as np

import pyvista as pv
from pyvista import examples

# Define some helpers - ignore these and
# use your own data!
def generate_points(subset=0.02):
    """A helper to make a 3D NumPy array of points (n_points by 3)"""
    dataset = examples.download_lidar()
    ids = np.random.randint(
        low=0,
        high=dataset.n_points - 1,
        size=int(dataset.n_points * subset))
    return dataset.points[ids]


points = generate_points()
# Print first 5 rows to prove its a numpy array (n_points by 3)
# Columns are (X Y Z)
points[0:5, :]

point_cloud = pv.PolyData(points)
point_cloud

np.allclose(points, point_cloud.points)

point_cloud.plot(eye_dome_lighting=True)

# Make data array using z-component of points array
data = points[:, -1]

# Add that data to the mesh with the name "uniform dist"
point_cloud["elevation"] = data

point_cloud.plot(render_points_as_spheres=True)

# Create random XYZ points
points = np.random.rand(100, 3)
# Make PolyData
point_cloud = pv.PolyData(points)

def compute_vectors(mesh):
    origin = mesh.center
    vectors = mesh.points - origin
    vectors = vectors / np.linalg.norm(
        vectors, axis=1)[:, None]
    return vectors


vectors = compute_vectors(point_cloud)
vectors[0:5, :]

point_cloud['vectors'] = vectors

arrows = point_cloud.glyph(
    orient='vectors',
    scale=False,
    factor=0.15,
)

# Display the arrows
plotter = pv.Plotter()
plotter.add_mesh(
    point_cloud,
    color='maroon',
    point_size=10.0,
    render_points_as_spheres=True)
plotter.add_mesh(
    arrows, color='lightblue')
# plotter.add_point_labels(
#            [point_cloud.center,], ['Center',],
#            point_color='yellow', point_size=20)

plotter.show_grid()
plotter.show()
  #+end_src

  #+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./pyVistaExample-1.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./pyVistaExample-2.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./pyVistaExample-3.PNG}
}
\caption{pyVista example.}
\label{fig:cont}%
\end{figure}
  #+end_src

      #+begin_export org
      #+attr_html: :width 500px
      #+attr_latex: :width 120px
      #+caption: pyVista example: aption
      #+name:   fig:pyVistaExample-1
      [[./pyVistaExample-1.PNG]]

      #+attr_html: :width 500px
      #+attr_latex: :width 120px
      #+caption: pyVista example:
      #+name:   fig:pyVistaExample-2
      [[./pyVistaExample-2.PNG]]

      #+attr_html: :width 500px
      #+attr_latex: :width 120px
      #+caption: pyVista example:
      #+name:   fig:pyVistaExample-3
      [[./pyVistaExample-3.PNG]]
      #+end_export

- [[https://github.com/openMVG/openMVG][openMVG]]

  OpenMVG provides an end-to-end 3D reconstruction from images framework compounded
  of libraries, binaries, and pipelines.

- [[https://github.com/cdcseacave/openMVS][openMVS]]

  OpenMVS addresses the last part of the photogrammetry chain-flow by providing a complete set of
  algorithms to recover the full surface of the scene to be reconstructed. The input is a set of
  camera poses plus the sparse point-cloud and the output is a textured mesh.

- [[https://github.com/mikedh/trimesh][trimesh]]

  Trimesh is a pure Python library for loading and using triangular meshes with an emphasis on
  watertight surfaces. The goal of the library is to provide a full featured and well tested
  /Trimesh object/ which allows for easy manipulation and analysis.

  Installation:

  #+begin_src shell
pip install pyglet
pip install trimesh[all]
  #+end_src

- [[https://tqdm.github.io/][tqdm]]

  Tqdm is a fast, extensible progress meter. It make your loops show a smart progress meter -
  just wrap any iterable with =tqdm(iterable)=, and you’re done!

  Installation:

  #+begin_src shell
pip install tqdm
  #+end_src

  Example:

  #+begin_src python
import time
from tqdm import tqdm

for i in tqdm(range(1000)):
    time.sleep(1/1000)
  #+end_src

- [[https://pandas.pydata.org/][pandas]]

  Pandas provides fast, flexible, and expressive data structures designed to make working with
  /relational/ or /labeled/ data both easy and intuitive. It aims to be the fundamental high-level
  building block for doing practical, real world data analysis in Python. Additionally, it has
  the broader goal of becoming the most powerful and flexible open source data analysis and
  manipulation tool available in any language. It is already well on its way towards this goal.

  Installation:

  #+begin_src shell
pip install pandas
  #+end_src

  Example:

  #+begin_src python
import numpy as np
import pandas as pd

series = pd.Series([1, 3, 5, np.nan, 6, 8])
dates  = pd.date_range("20130101", periods=6)
frame  = pd.DataFrame(
             np.random.randn(6, 4),
             index = dates,
             columns = list("ABCD"))
frame2 = pd.DataFrame(
    {

        "A": 1.0,
        "B": pd.Timestamp("20130102"),
        "C": pd.Series(
                   1,
                   index = list(range(4)),
                   dtype="float32"),
        "D": np.array([3] * 4, dtype="int32"),
        "E": pd.Categorical([
                              "test",
                              "train",
                              "test",
                              "train"
                            ]),
        "F": "foo",
    }
)

  #+end_src

- [[https://github.com/boto/boto3][boto3]]

  Boto3 is the [[https://aws.amazon.com/de/what-is-aws/][Amazon Web Services (AWS)]] Software Development Kit (SDK) for Python, which allows Python
  developers to write software that makes use of services like /Amazon S3/ and /Amazon EC2/.

  #+begin_src shell
python -m pip install boto3
  #+end_src

  #+begin_example
~/.aws/credentials

[default]
aws_access_key_id = YOUR_KEY
aws_secret_access_key = YOUR_SECRET

~/.aws/config

[default]
region=us-east-1
  #+end_example

  #+begin_src python
import boto3

s3 = boto3.resource('s3')

for bucket in s3.buckets.all():
    print(bucket.name)
  #+end_src

- [[https://scikit-learn.org/stable/index.html][scikit-learn]]

  scikit-learn is a Python module for machine learning built on top of SciPy.

  #+caption: Example: scikit-learn-kmean++.py
  #+begin_src python
from sklearn.cluster import kmeans_plusplus
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Generate sample data
n_samples    = 4000
n_components = 4

X, y_true = make_blobs(
    n_samples    = n_samples,
    centers      = n_components,
    cluster_std  = 0.60,
    random_state = 0
)

X = X[:, ::-1]

# Calculate seeds from kmeans++
centers_init, indices = kmeans_plusplus(
    X,
    n_clusters   = 4,
    random_state = 0)

# Plot init seeds along side sample data
plt.figure(1)
colors = [
    "#4EACC5",
    "#FF9C34",
    "#4E9A06",
    "m"
]

for k, col in enumerate(colors):
    cluster_data = y_true == k
    plt.scatter(
        X[cluster_data, 0],
        X[cluster_data, 1],
        c = col,
        marker = ".",
        s = 10)

plt.scatter(
    centers_init[:, 0],
    centers_init[:, 1],
    c = "b",
    s = 50)

plt.title("K-Means++ Initialization")
plt.xticks([])
plt.yticks([])

plt.show()
  #+end_src

  #+begin_export org
  #+attr_html: :width 500px
  #+end_export
  #+attr_latex: :width 120px
  #+caption: caption.
  #+name:   fig:example
  [[./scikit-learn-1.png]]

* Surface Reconstruction

/Surface reconstruction/ is the process of generating /polygon meshes/ or /CAD models/ from 3D point cloud data.
It is important to generate /watertight/ surfaces[[cite:&taubinDesktop3DScanning]].

The objects scanned are solid, with a well-defined boundary surface separating the inside from the outside.
The triangulation-based 3D scanners produce a finite sampling scheme. The so-called /point cloud/, is a dense
collection of surface samples. Since point clouds do not constitute surfaces, they cannot be used to
determine which 3D points are inside or outside of the solid object.

In addition to the 3D point locations, the 3D scanning methods are often able to estimate a color per
point, as well as a surface normal vector. Some methods are able to measure both color and surface normal,
and some are able to estimate other parameters which can be used to describe more complex material properties
used to generate complex renderings.

** Point Cloud Data                                                :publish:

Among other things the following lists the important data that can be found in point cloud data.

- 3D locations of the scanned points,
- colors associated with a point,
- surface normals associated with the local surace region of the points,
- estimations of other important material properties.

** Visualization                                                   :publish:

A well-established technique to render dense point clouds is point splatting. Each point is
regarded as an oriented disk in 3D, with the orientation determined by the surface normal
evaluated at each point, and the radius of the disk usually stored as an additional parameter
per vertex. As a result, each point is rendered as an ellipse. The color is determined by the
color stored with the point, the direction of the normal vector, and the illumination model.
The radii are chosen so that the ellipses overlap, resulting in the perception of a continuous
surface being rendered.

** Merging Point Clouds                                            :publish:

To produce a complete representation of a surface, multiple scans taken from various points
of view must be integrated to produce a point cloud with sufficient sampling density over the
whole visible surface of the object being scanned.

*** Computing Rigid Body Matching Transformations

The main challenge to merging multiple scans is that each scan is produced with respect to
a different coordinate system.

When repositioning is performed by hand, the matching transformations are not known and
need to be estimated from point correspondences.

Consider the problem of computing the rigid body transformation $q = R p + T$ in order to align
two shapes from two sets of $N$ corresponding points, ${p_1,\dots,p_N}$ and ${q_1,\ldots,q_N}$.
That is, we are looking for a /rotation matrix/ $R$ and a /translation vector/ $T$ so that

\begin{equation}
  \begin{array}{ccc}
    q_1 = R p_1 + T & \dots & q_N = R p_N + T
  \end{array}
\end{equation}

Being able to compute the unknown matching transformation in closed form is a fundamental operation.
This /registration/ problem[[cite:&enwiki:1095051621]] is generally not solvalable due to measurements
errors. A common approach
is to seek a least-squares solution, that is a closed-form solution for minimizing the mean
squared error.

\begin{equation}
\label{eqn:LSS1}
\phi(R,T) = \frac{1}{N}\sum_{i=1}^{N} \| Rp_i + T - qi\|^2
\end{equation}

which yields a quadratic function of /12 components/ in $R$ and $T$; however, since $R$ is restricted to be
a valid rotation matrix, there exist additional constraints on $R$.

Since the variable $T$ is unconstrained, a closed-form solution for $T$, as a function of $R$, can be
found by solving the linear system of equations resulting from differentiating the previous expression
with respect to $T$.

\begin{equation}
  \begin{array}{ccc}
     \frac{1}{2}\frac{\partial \phi}{\partial T} = \frac{1}{N}\sum\limits_{i=1}^{N} ( Rp_i + T - qi) = 0 & \Rightarrow & T = \bar{q} - R\bar{p}
  \end{array}
\end{equation}

$\bar{p}$ and $\bar{q}$ are the /geometric centroids/ of the two sets of matching points, i.e.

\begin{equation}
  \begin{array}{ccc}
     \bar{p} = \frac{1}{N}\sum\limits_{i=1}^{N} p_i & \text{and} & \bar{q} = \frac{1}{N}\sum\limits_{i=1}^{N} q_i.
  \end{array}
\end{equation}

Substituting for $T$ in Equation \ref{eqn:LSS1} yields

\begin{equation}
\phi(R) = \frac{1}{N}\sum_{i=1}^{N} \| R(p_i - \bar{p}) - (qi - \bar{q})\|^2.
\end{equation}

This equation can be expanded to

\begin{equation}
\begin{split}
\phi(R) = &
  \frac{1}{N}\sum_{i=1}^{N} \| p_i - \bar{p})\|^2 \\
  & - \frac{2}{N}\sum_{i=1}^{N} (qi - \bar{q})^T R(p_i - \bar{p}) \\
  & + \frac{1}{N}\sum_{i=1}^{N} \| q_i - \bar{q})\|^2,
\end{split}
\end{equation}

since $||Rv||^2=||v||^2$ for any vector $v$.
As the first and last terms do not depend on $R$, maximizing this expression is equivalent to maximizing

\begin{equation}
\eta(R) = \frac{1}{N}\sum_{i=1}^{N} (qi - \bar{q})^T R(p_i - \bar{p}) = trace(RM)
\end{equation}

where $M$ is the $3 \times 3$ matrix

\begin{equation}
M = \frac{1}{N}\sum_{i=1}^{N} (p_i - \bar{p})(qi - \bar{q})^T.
\end{equation}

Considering the singular value decomposition (SVD) $M = U \Delta V^T$, where $U$ and $V$ are orthogonal $3 \times 3$ matrices,
and $\Delta$ is a diagonal $3 \times 3$ matrix with elements $\delta_1 \le \delta_2 \le \delta_3 \le 0$. Substituting, we find

\begin{equation}
\begin{split}
trace(RM) & = trace(RU\Delta V^T) \\
& = trace((V^T R U)\Delta) \\
& = trace(W \Delta),
\end{split}
\end{equation}

where $W = V^T R U$ is orthogonal. If we expand this expression, we obtain

\begin{equation}
trace(W\Delta) = w_{11}\delta_1 + w_{22}\delta_2 +  w_{33}\delta_3 \le \delta_1 + \delta_2 + \delta_3,
\end{equation}

where $W = (w_{ij})$. The last inequality is true because the components of an orthogonal matrix cannot
be larger than one. Note that the last inequality is an equality only if $w_{11}=w_{22}=w_{33} = 1$,
which is only the case when $W=I$.
It follows that if $V^T U$ is a rotation matrix, then $R = V^T U$ is the minimizer of our original problem.
The matrix $V^T U$ is an orthogonal matrix, but it may not have a negative determinant. In that case,
an upper bound for $trace(W\Delta)$, with $W$ restricted to have a negative determinant,
is achieved for $W = J$, where

\begin{equation}
J = \left(
\begin{array}{ccc}
1 & 0 &  0 \\
0 & 1 &  0 \\
0 & 0 & -1
\end{array}
\right)
\end{equation}

In this case it follows that the solution to our problem is $R = V^T U$.

*** The Iterative Closest Point (ICP) Algorithm

The /Iterative Closest Point (ICP)[[cite:&enwiki:1095051621;&rusinkiewicz_efficient_2001]]/ is an algorithm employed to
match two surface representations, such as points clouds or polygon meshes. This matching algorithm is used to
reconstruct 3D surfaces by registering and merging multiple scans. The algorithm is straightforward and can be
implemented in real-time.

ICP iteratively estimates the transformation (i.e., translation and rotation) between two geometric data sets.
The algorithm takes as input two data sets, an initial estimate for the transformation, and an additional
criterion for stopping the iterations. The output is an improved estimate of the matching transformation.
The algorithm comprises the following steps:

- Select points from the first shape.
- Associate points, by nearest neighbor, with those in the second shape.
- Estimate the closed-form matching transformation using the method derived in the previous section.
- Transform the points using the estimated parameters.
- Repeat previous steps until the stopping criterion is met.

The algorithm can be generalized to solve the problem of registering multiple scans. Each scan has an
associated rigid body transformation which will register it with respect to the rest of the scans,
regarded as a single rigid object. An additional external loop must be added to the previous steps
to pick one transformation to be optimized with each pass, while the others are kept constant - either
going through each of the scans in sequence, or randomizing the choice.

** Surface Reconstruction from Point Clouds                        :publish:

/Watertight surfaces/ partition space into two disconnected regions so that every line segment joining a
point in one region to a point in the other must cross the dividing surface. In this section we discuss
methods to reconstruct watertight surfaces from point clouds.

*** Continuous Surfaces

In mathematics surfaces are represented in parametric or implicit form. Because of its wide spread
use, we focus our remaining discussion on implicit surfaces.

An implicit surface is defined as a set

\begin{equation}
S = \left\{
  p \in \mathbb{R}^3 : f(p) = \lambda \in \mathbb{R}
\right\}
\end{equation}

of a continuous function $f:V\to\mathbb{R}$, where $V$ is an open subset in 3D.
These functions are most often smooth or piecewise smooth.
Implicit surfaces are called /watertight/ because they partition space into the
two disconnected sets of points, one where $f(p) > \lambda$ and a second where $f(p) < \lambda$.
Since the function $f$ is continuous, every line segment joining a point in one region to a point
in the other must cross the dividing surface. When the boundary surface of a solid object is described
by an implicit equation, one of these two sets describes the inside of the object, and the other one
the outside. Since the implicit function can be evaluated at any point in 3D space, it is also referred
to as a scalar field.

*** Discrete Surfaces

A discrete surface is defined by a finite number of parameters. We only consider here polygon meshes.
Polygon meshes are composed of /geometry/ and /topological connectivity/.
The geometry includes vertex coordinates, normal vectors, and colors (and possibly texture coordinates).
The connectivity is represented in various ways. E.g. with polygon faces represented as loops of
vertex coordinate vectors. If two or more faces share a vertex, the vertex coordinates are repeated
as many times as needed.

*** Isosurfaces

An isosurface is a polygonal mesh surface representation produced by an isosurface algorithm.
An isosurface algorithm constructs a polygonal mesh approximation of a smooth implicit surface $S$
within a bounded three-dimensional volume, from samples of a defining function $f(x)$ evaluated on
the vertices of a volumetric grid. Marching Cubes[[cite:&lorensen__nodate;&newman_survey_2006;&mcPaulBourke]]
is one of such algorithms
that operate on function values provided at the vertices of /hexahedral grids/. Another family of
isosurface algorithms operate on functions evaluated at the vertices of tetrahedral grids[[cite:&Doi1991AnEM]]
Usually, no additional information about the function is provided, and various interpolation schemes
are used to evaluate the function within grid cells, if necessary.

*** Isosurface Construction Algorithms

An isosurface algorithm producing a /polygon soup/ output must solve four key problems:

- determining the quantity and location of isosurface vertices within each cell,
- determining how these vertices are connected forming isosurface faces,
- and determining globally consistent face orientations.
- identifying isosurface vertices lying on vertices and edges of the volumetric grid.

Researchers have proposed various solutions and design decisions (e.g., cell types,
adaptive grids, topological complexity, interpolant order) to address these four problems.
The well-known Marching Cubes[[cite:&lorensen__nodate;&newman_survey_2006;&mcPaulBourke]]
algorithm uses a fixed hexahedral grid
(i.e., cube cells) with linear interpolation to find zero-crossings along the edges of the
grid. These are the vertices of the isosurface mesh.
Second, polygonal faces are added connecting these vertices using a table. The crucial
observation made with Marching Cubes is that the possible connectivity of triangles
in a cell can be computed independently of the function samples and stored in a table.

*** Various Reconstruction Algorithms

- Alpha Shapes Algorithm[[cite:&edelsbrunner_shape_1983-1]][[cite:&fischer_alpha_shapes]] :: This algorithm is a generalization of a convex hull algorithm.
- Ball Pivoting Algorithm (BPA)[[cite:&bernardini_ball-pivoting_1999]] :: This algorithm is a generalization of a convex hull algorithm.
- Poisson Surface Reconstruction[[cite:&kazhdan_poisson_nodate]] :: Optimization problem based algorithm.
- Recurrent Edge Inference Network[[cite:&daroya_rein_2020]] - REIN :: AI based algorithm to generate meshes from point clouds.

** Open3D Surface Reconstruction

- Alpha Shapes Algorithm
- Ball Pivoting Algorithm (BPA)
- Poisson Surface Reconstruction

*** Alpha Shapes Algorithm                                        :publish:

The Alpha Shapes Algorithm[[cite:&edelsbrunner_shape_1983-1]][[cite:&fischer_alpha_shapes]] is a generalization of a convex hull.
One can intuitively think of an alpha shape as the following[[cite:&fischer_alpha_shapes]]:

#+begin_quote
Imagine a huge mass of ice cream containing the points $S$ as hard chocolate pieces. Using one
of these sphere-formed ice cream spoons we carve out all parts of the ice cream block we can
reach without bumping into chocolate pieces, thereby even carving out holes in the inside
(e.g., parts not reachable by simply moving the spoon from the outside). We will eventually
end up with a (not necessarily convex) object bounded by caps, arcs and points. If we now
straighten all round faces to triangles and line segments, we have an intuitive description
of what is called the /alpha shape/ of $S$.
#+end_quote

The following example loads the /Bunny mesh/ and samples 2000 points from this model in order to
create a point cloud. Then this point cloud is used to reconstruct the surface using the /Alpha Shape/ algorithm.
The algorithm is governed by tradeoff parameter /alpha/. It is not easy to get a matching /alpha/ parameter
for defect free reconstruction of the /Bunny mesh/.

#+begin_src python
import numpy as np
import open3d as o3d

if  __name__=='__main__':
    print("Alpha Shape Algorithm")

    bunny_mesh = o3d.data.BunnyMesh()
    mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)

    pcd = mesh.sample_points_poisson_disk(3000)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries([pcd])

    for alpha in np.logspace(np.log10(0.5),
                         np.log10(0.01),
                         num=4):
        print(f"alpha={alpha:.3f}")

        mesh = o3d.geometry. \
            TriangleMesh.    \
            create_from_point_cloud_alpha_shape(pcd, alpha)

        mesh.compute_vertex_normals()

        o3d.visualization.draw_geometries(
            [pcd, mesh],
            mesh_show_back_face=True)
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./bunnySampled1.png}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./bunnySampled2.png}
}
\caption{Sampled Bunny mesh with 3000 points.}
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+ATTR_LaTeX: :height 0.2\textwidth :center
#+attr_html: :width 500px
#+CAPTION: Sampled Bunny mesh with 3000 points.
 [[./bunnySampled1.png]]
#+ATTR_LaTeX: :height 0.2\textwidth :center
#+attr_html: :width 500px
#+CAPTION: Sampled Bunny mesh with 3000 points.
[[./bunnySampled2.png]]
#+END_center
#+END_EXPORT

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][alpha=0.500]{
  \includegraphics[width=.3\linewidth]{./AlphaShape1.png}
}
\qquad
\subfloat[][alpha=0.136]{
  \includegraphics[width=.3\linewidth]{./AlphaShape2.png}
}
\qquad
\subfloat[][alpha=0.0370]{
  \includegraphics[width=.3\linewidth]{./AlphaShape3.png}
}
\qquad
\subfloat[][alpha=0.0100]{
  \includegraphics[width=.3\linewidth]{./AlphaShape4.png}
}
\caption{Alpha Shape reconstruction of the sampled Bunny mesh for various alpha parameters.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: alpha = 0.500
 [[./AlphaShape1.png]]
#+attr_html: :width 500px
#+CAPTION: alpha = 0.136
[[./AlphaShape2.png]]
#+attr_html: :width 500px
#+CAPTION: alpha = 0.037
[[./AlphaShape3.png]]
#+attr_html: :width 500px
#+CAPTION: alpha = 0.010
[[./AlphaShape4.png]]
#+END_center
#+END_EXPORT

*** Ball Pivoting Algorithm                                       :publish:

The Ball Pivoting Algorithm[[cite:&bernardini_ball-pivoting_1999]] is a surface reconstruction
method which is related to the Alpha Shapes algorithm.
Intuitively, think of a 3D ball with a given radius that we drop on the point cloud. If it
hits any 3 points (and it does not fall through those 3 points) it creates a triangles.
Then, the algorithm starts pivoting from the edges of the existing triangles and every time
it hits 3 points where the ball does not fall through we create another triangle.

The method accepts a list of /radii/ as parameter that corresponds to the radii of the
individual /balls/ that are pivoted on the point cloud. This algorithm assumes that the
point cloud has normals.

#+begin_src python
import numpy as np
import open3d as o3d

if  __name__=='__main__':
    print("Ball Pivoting Algorithm")

    bunny_mesh = o3d.data.BunnyMesh()
    mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)
    mesh.compute_vertex_normals()

    pcd = mesh.sample_points_poisson_disk(3000)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries([pcd])

    radii = [0.005, 0.01, 0.02, 0.04]

    mesh = o3d.geometry.TriangleMesh.\
                create_from_point_cloud_ball_pivoting(
                    pcd,
                    o3d.utility.DoubleVector(radii))

    o3d.visualization.draw_geometries(
        [pcd, mesh],
        mesh_show_back_face=True)
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Ball pivoting reconstruction of the sampled Bunny mesh.
#+NAME:   fig:BallPivoting1
[[./BallPivoting1.png]]

*** Poisson Surface Reconstruction                                :publish:

The Poisson Surface Reconstruction[[cite:&kazhdan_poisson_nodate]] solves a regularized optimization
problem to obtain a smooth surface. For this reason, Poisson surface reconstruction can be preferable
to the methods mentioned above, as they produce non-smooth results since the points of the /point cloud/
are also the vertices of the resulting triangle mesh without any modifications.

An important parameter of the algorithm is /depth/ that defines the depth of the /octree/ used for
the surface reconstruction and hence implies the resolution of the resulting triangle mesh.
A higher /depth/ value means a mesh with more details.

This algorithm also assumes exiting normals for the /point cloud/.

#+begin_src python
import numpy as np
import open3d as o3d

if  __name__=='__main__':
    print("Poisson Surface Reconstruction Algorithm")

    bunny_mesh = o3d.data.BunnyMesh()
    mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)

    mesh.compute_vertex_normals()

    pcd = mesh.sample_points_poisson_disk(3000)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries([pcd, mesh])

    depth = 9

    mesh, densities = o3d.geometry.TriangleMesh.\
                create_from_point_cloud_poisson(
                    pcd,
                    depth=depth)

    mesh.compute_vertex_normals()
    mesh.paint_uniform_color([0.7, 0.7, 0.7])

    o3d.visualization.draw_geometries(
        [pcd, mesh],
        mesh_show_back_face=False)
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: Poisson surface reconstruction of the sampled Bunny mesh.
#+NAME:   fig:PoissonSurfaceReconstruction1
[[./PoissonSurfaceReconstruction1.png]]

In the following example a real /point cloud/ of an eagel is used.
Note, that /Poisson surface reconstruction/ will also create triangles
in areas of low point density, and even extrapolates into some areas.

#+begin_src python
import numpy as np
import open3d as o3d

if  __name__=='__main__':
    print("Poisson Surface Reconstruction Algorithm")

    eagle = o3d.data.EaglePointCloud()
    pcd = o3d.io.read_point_cloud(eagle.path)

    R = pcd.get_rotation_matrix_from_xyz((np.pi, -np.pi / 4, 0))
    pcd.rotate(R, center=(0, 0, 0))

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries([pcd])

    depth = 9

    mesh, densities = o3d.geometry.TriangleMesh.\
                create_from_point_cloud_poisson(
                    pcd,
                    depth=depth)

    o3d.visualization.draw_geometries(
        [mesh],
        mesh_show_back_face=False)
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./eaglePCL1.png}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./eaglePSR1.png}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./eaglePCL2.png}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./eaglePSR2.png}
}
\caption{Poisson surface reconstruction of the eagle point cloud.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: The raw eagle point cloud.
 [[./eaglePCL1.png]]
#+attr_html: :width 500px
#+CAPTION: Poisson surface reconstruction eagle point cloud.
[[./eaglePSR1.png]]
#+attr_html: :width 500px
#+CAPTION: The raw eagle point cloud.
[[./eaglePCL2.png]]
#+attr_html: :width 500px
#+CAPTION: Poisson surface reconstruction of the eagle point cloud.
[[./eaglePSR2.png]]
#+END_center
#+END_EXPORT

Beside of the /mesh/ an additional /densities/ value is returned for each vertex.
A low density value means that the vertex is only supported by a low number
of points from the input point cloud.

We can visualize these as well. We can further use the density values to remove
vertices and triangles that have a low support. In the code below we
remove all vertices (and connected triangles) that have a lower density value
than the 0.01 quantile of all density values.

#+begin_src python
import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt

if  __name__=='__main__':
    print("Poisson Surface Reconstruction Algorithm")

    eagle = o3d.data.EaglePointCloud()
    pcd = o3d.io.read_point_cloud(eagle.path)

    R = pcd.get_rotation_matrix_from_xyz((np.pi, -np.pi / 4, 0))
    pcd.rotate(R, center=(0, 0, 0))

    o3d.visualization.webrtc_server.enable_webrtc()

    depth = 9

    mesh, densities = o3d.geometry.TriangleMesh.\
                create_from_point_cloud_poisson(
                    pcd,
                    depth=depth)

    densities = np.asarray(densities)

    density_colors = plt.get_cmap('plasma')(
            (densities - densities.min()) /
            (densities.max() - densities.min())
        )

    density_colors = density_colors[:,:3]

    density_mesh = o3d.geometry.TriangleMesh()

    density_mesh.vertices  = mesh.vertices
    density_mesh.triangles = mesh.triangles

    density_mesh.triangle_normals = mesh.triangle_normals
    density_mesh.vertex_colors    = \
        o3d.utility.Vector3dVector(density_colors)

    o3d.visualization.draw_geometries([density_mesh])
    o3d.visualization.draw_geometries([mesh])

    vertices_to_remove = densities < np.quantile(densities, 0.01)
    mesh.remove_vertices_by_mask(vertices_to_remove)

    o3d.visualization.draw_geometries([mesh])
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Point cloud]{
  \includegraphics[width=.3\linewidth]{./eaglePCL1.png}
}
\qquad
\subfloat[][Poisson surface reconstruction]{
  \includegraphics[width=.3\linewidth]{./eaglePSR1.png}
}
\qquad
\subfloat[][Vertices densities]{
  \includegraphics[width=.3\linewidth]{./eagleDVZ1.png}
}
\qquad
\subfloat[][Low support removable]{
  \includegraphics[width=.3\linewidth]{./eaglePSR3.png}
}
\caption{Poisson surface reconstruction of the eagle point cloud.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: The raw eagle point cloud.
 [[./eaglePCL1.png]]
#+attr_html: :width 500px
#+CAPTION: Poisson surface reconstruction eagle point cloud.
[[./eaglePSR1.png]]
#+attr_html: :width 500px
#+CAPTION: Poisson surface reconstructed vertices density support.
[[./eagleDVZ1.png]]
#+attr_html: :width 500px
#+CAPTION: Poisson surface reconstruction with low support vertices removed.
[[./eaglePSR3.png]]
#+END_center
#+END_EXPORT

*** Normal Estimation                                             :publish:

Not all point clouds already come with associated normals. Open3D can be used to estimate point cloud normals,
by fitting a local plane to each of the point cloud vertices. The estimated normals might not be consistently oriented,
and therefore an additional processing step is necessary.

#+begin_src python
import numpy as np
import open3d as o3d

if  __name__=='__main__':
    print("Normal Estimation")

    bunny_mesh = o3d.data.BunnyMesh()
    mesh = o3d.io.read_triangle_mesh(bunny_mesh.path)

    pcd = mesh.sample_points_poisson_disk(3000)

    pcd.normals = o3d.utility.Vector3dVector(
        np.zeros((1, 3)))  # invalidate existing normals

    pcd.estimate_normals()

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries(
        [pcd],
        point_show_normal=True)

    pcd.orient_normals_consistent_tangent_plane(100)

    o3d.visualization.draw_geometries(
        [pcd],
        point_show_normal=True)
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Estimated]{
  \includegraphics[width=.3\linewidth]{./NormalEstimation1.png}
}
\qquad
\subfloat[][Corrected]{
  \includegraphics[width=.3\linewidth]{./NormalEstimation2.png}
}
\caption{Normal estimation and correction of a point cloud.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: Normal estimation of a point cloud.
 [[./NormalEstimation1.png]]
#+attr_html: :width 500px
#+CAPTION: Correction of the estimated normals.
[[./NormalEstimation2.png]]
#+END_center
#+END_EXPORT

*** Voxel Downsampling                                            :publish:

/Voxel downsampling/ uses a regular voxel grid to create a uniformly downsampled
/point cloud/ from an input point cloud. It is often used as a pre-processing step
for many point cloud processing tasks. The algorithm operates in two steps:

- Points are bucketed into voxels.
- Each occupied voxel generates exactly one point by averaging all points inside.

#+begin_src python
import numpy as np
import open3d as o3d
import pye57

if  __name__=='__main__':
    print("Loading Brotfabrik.e57 with pye57 and Open3D")

    e57 = pye57.E57("b:\PointClouds\Data\Brotfabrik.e57")
    data = e57.read_scan(
                0,
                colors=True,
                ignore_missing_fields=True)

    x = np.array(data["cartesianX"])
    y = np.array(data["cartesianY"])
    z = np.array(data["cartesianZ"])

    r = np.array(data["colorRed"])
    g = np.array(data["colorGreen"])
    b = np.array(data["colorBlue"])

    r = np.divide(r, 256.0)
    g = np.divide(g, 256.0)
    b = np.divide(b, 256.0)

    xyz = np.zeros((np.size(x), 3))

    xyz[:, 0] = np.reshape(x, -1)
    xyz[:, 1] = np.reshape(y, -1)
    xyz[:, 2] = np.reshape(z, -1)

    rgb = np.zeros((np.size(r), 3))

    rgb[:, 0] = np.reshape(r, -1)
    rgb[:, 1] = np.reshape(g, -1)
    rgb[:, 2] = np.reshape(b, -1)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(xyz)
    pcd.colors = o3d.utility.Vector3dVector(rgb)

    pcd = pcd.voxel_down_sample(voxel_size=0.5)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries(
        [pcd],
        point_show_normal=False,
        zoom=1.0,
        front=[0.5257, 3.2125, 0.6795],
        lookat=[0.0, 0.0, 0.0],
        up=[0.0, 0.0, 1.0])

    pcd.normals = o3d.utility.Vector3dVector(
        np.zeros((1, 3)))

    pcd.estimate_normals(
        search_param = o3d.geometry.\
            KDTreeSearchParamHybrid(radius=0.1, max_nn=30)
    )

    o3d.visualization.draw_geometries(
        [pcd],
        point_show_normal=True,
        zoom=1.0,
        front=[0.5257, 3.2125, 0.6795],
        lookat=[0.0, 0.0, 0.0],
        up=[0.0, 0.0, 1.0])
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Voxel downsampled]{
  \includegraphics[width=.3\linewidth]{./VoxelDown1.PNG}
}
\qquad
\subfloat[][Normals estimated]{
  \includegraphics[width=.3\linewidth]{./VoxelDownNormals1.PNG}
}
\caption{Voxel downsampling with subsequentely normal estimation.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: Voxel downsampling of the Brotfabrik.e57 point cloud.
 [[./VoxelDown1.PNG]]
#+attr_html: :width 500px
#+CAPTION: Normal estimation of the Voxel downsampled Brotfabrik.e57 point cloud.
[[./VoxelDownNormals1.PNG]]
#+END_center
#+END_EXPORT

*** Point Cloud Cropping                                          :publish:

Open3D allows the /cropping/ of point cloud data. The cropping volume is
specified in a /JSON/ file in form of a polygon selection area

#+begin_src json
{
	"axis_max" :  4.022921085357666,
	"axis_min" : -0.763413667678833,
	"bounding_polygon" :
	[
		[ 2.6509309513852526, 0.0, 1.6834473132326844 ],
		[ 2.5786428246917148, 0.0, 1.6892074266735244 ],
		[ 2.4625790337552154, 0.0, 1.6665777078297999 ],
		[ 2.2228544982251655, 0.0, 1.6168160446813649 ],
		[ 2.1669932060014130, 0.0, 1.6115495157201662 ],
		[ 2.1167895865303286, 0.0, 1.6257706054969348 ],
		[ 2.0634657721747383, 0.0, 1.6230216586245390 ],
		[ 2.0568612343437236, 0.0, 1.5853892911207643 ],
		[ 2.1605399001237027, 0.0, 0.9622899325508302 ],
		[ 2.1956669387205228, 0.0, 0.9557274604978507 ],
		[ 2.2191318790575583, 0.0, 0.8873444998210875 ],
		[ 2.2484881847925919, 0.0, 0.8704280726701363 ],
		[ 2.6891234157295827, 0.0, 0.9414067798896760 ],
		[ 2.7328692490470647, 0.0, 0.9877574067484025 ],
		[ 2.7129337547575547, 0.0, 1.0398850034649203 ],
		[ 2.7592174072415405, 0.0, 1.0692940558509485 ],
		[ 2.7689216419453428, 0.0, 1.0953914441371593 ],
		[ 2.6851455625455669, 0.0, 1.6307334122162018 ],
		[ 2.6714776099981239, 0.0, 1.6755246570889970 ],
		[ 2.6579576128816544, 0.0, 1.6819127849749496 ]
	],
	"class_name" : "SelectionPolygonVolume",
	"orthogonal_axis" : "Y",
	"version_major" : 1,
	"version_minor" : 0
}
#+end_src

#+begin_src python
import numpy as np
import open3d as o3d
import pye57

if  __name__=='__main__':
    print("Cropping in Point Cloud")

    demo_crop_data = o3d.data.DemoCropPointCloud()

    pcd = o3d.io.read_point_cloud(
                    demo_crop_data.point_cloud_path)

    vol = o3d.visualization.\
                read_selection_polygon_volume(
                    demo_crop_data.cropped_json_path)

    chair = vol.crop_point_cloud(pcd)

    o3d.visualization.webrtc_server.enable_webrtc()

    o3d.visualization.draw_geometries(
            [pcd],
            zoom=0.7,
            front=[0.5439, -0.2333, -0.8060],
            lookat=[2.4615, 2.1331, 1.338],
            up=[-0.1781, -0.9708, 0.1608])

    o3d.visualization.draw_geometries(
            [chair],
            zoom=0.7,
            front=[0.5439, -0.2333, -0.8060],
            lookat=[2.4615, 2.1331, 1.338],
            up=[-0.1781, -0.9708, 0.1608])
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./DemoScene1.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./Chair1.PNG}
}
\caption{Point cloud cropping.}
\label{fig:cont}%
\end{figure}
#+end_src

#+BEGIN_EXPORT org
#+BEGIN_center
#+attr_html: :width 500px
#+CAPTION: Point cloud used for cropping.
 [[./DemoScene1.PNG]]
#+attr_html: :width 500px
#+CAPTION: Cropped chair from point cloud.
[[./Chair.PNG]]
#+END_center
#+END_EXPORT

*** DBSCAN Clustering                                             :publish:

Open3D allows to group /local point cloud clusters/ together. It implements DBSCAN[[cite:&esterDensityBased1996]] that is a density based clustering algorithm.
The algorithm requires two parameters:

- /eps/ defines the distance to neighbors in a cluster and
- /min_points/ defines the minimum number of points required to form a cluster.

The algorithm returns labels, where the label -1 indicates noise.

The algorithm precomputes all neighbors in the epsilon radius for all points.
This can require a lot of memory if the chosen epsilon is too large.

#+begin_src python
import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt

if  __name__=='__main__':
    print("DBSCAN clustering in Point Cloud")

    ply_point_cloud = o3d.data.PLYPointCloud()
    pcd = o3d.io.read_point_cloud(
                    ply_point_cloud.path)


    with o3d.utility.VerbosityContextManager(
            o3d.utility.VerbosityLevel.Debug) as cm:
        labels = np.array(
            pcd.cluster_dbscan(
                    eps=0.02,
                    min_points=10,
                    print_progress=True)
        )

        max_label = labels.max()

        print(f"point cloud has {max_label + 1} clusters")

        colors = plt.get_cmap("tab20")(
            labels / (max_label if max_label > 0 else 1))

        colors[labels < 0] = 0

        pcd.colors = o3d.utility.Vector3dVector(
                                     colors[:,:3])

        o3d.visualization.webrtc_server.enable_webrtc()
        o3d.visualization.draw_geometries(
                [pcd],
                zoom=0.455,
                front=[-0.4999, -0.1659, -0.8499],
                lookat=[2.1813, 2.0619, 2.0999],
                up=[0.1204, -0.9852, 0.1215])
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: DBSCAN clustering of point cloud.
#+NAME:   fig:clustering1
[[./Clustering1.png]]

*** RANSAC Plane Segmentation                                     :publish:

Open3D also supports /segmententation of geometric primitives/ from point clouds using the RANSACg[[cite:&fischler_random_1981;&enwiki:1091089831]] algorithm.
To find the /plane with the largest support/ in the point cloud, the Open3D function /segment_plane/ is used, which takes three arguments:

- /distance_threshold/ defines the maximum distance a point can have to an estimated plane to be considered an inlier.
- /ransac_n/ defines the number of points that are randomly sampled to estimate a plane.
- /num_iterations/ defines how often a random plane is sampled and verified.

The function then returns the plane as $(a,b,c,d)$ such that for each point $(x,y,z)$ on the plane we
have $ax+by+cz+d=0$. The function further returns a list of indices of the inlier points.

#+begin_src python
import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt

if  __name__=='__main__':
    print("RANSAC segmentation in Point Cloud")

    pcd_point_cloud = o3d.data.PCDPointCloud()
    pcd = o3d.io.read_point_cloud(pcd_point_cloud.path)

    plane_model, inliers = pcd.segment_plane(
            distance_threshold=0.01,
            ransac_n=3,
            num_iterations=1000)

    [a, b, c, d] = plane_model

    print(f"Plane equation: "
          f"{a:.2f}x + {b:.2f}y + {c:.2f}z + {d:.2f} = 0")

    inlier_cloud = pcd.select_by_index(inliers)
    inlier_cloud.paint_uniform_color([1.0, 0, 0])

    outlier_cloud = pcd.select_by_index(
        inliers,
        invert=True)

    o3d.visualization.webrtc_server.enable_webrtc()
    o3d.visualization.draw_geometries(
            [inlier_cloud, outlier_cloud],
            zoom=0.8,
            front=[-0.4999, -0.1659, -0.8499],
            lookat=[2.1813, 2.0619, 2.0999],
            up=[0.1204, -0.9852, 0.1215])
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+end_export
#+attr_latex: :width 220px
#+CAPTION: RANSAC segmentation of a point cloud.
#+NAME:   fig:segmentation1
[[./Segmentation1.png]]

*** Automatic Segmentation with RANSAC and DBSCAN                 :publish:

The following algorithms automatically segments the given
kitchen scene into point cloud clusters.

#+caption: Example: AutoSegmentation6.py
#+begin_src python
import numpy as np
import open3d as o3d
import matplotlib.pyplot as plt

if  __name__=='__main__':
    print("Refined RANSAC with Euclidean clustering, final.")

    camera = {
        'zoom'   : 0.32,
        'front'  : [0.30, 0.94, 0.154],
        'lookat' : [-3.96, -0.06, -0.28],
        'up'     : [-0.04, -0.14, 0.99]
    }

    ply_path = r'b:\PointClouds\Data\TLS_kitchen.ply'
    pcd = o3d.io.read_point_cloud(ply_path)

    pcd.estimate_normals(
        search_param = o3d.geometry.\
            KDTreeSearchParamHybrid(
                radius = 0.1,
                max_nn = 16),
                fast_normal_computation = True
    )

    pcd.paint_uniform_color([0.6, 0.6, 0.6])

    o3d.visualization.draw_geometries([pcd], **camera)

    #
    # RANSAC loop for multiple planar shapes detection
    #
    segment_models = {}
    segments       = {}

    max_plane_idx  = 20
    d_threshold    = 0.01

    rest = pcd  # the original kitchen point cloud

    for i in range(max_plane_idx):
        colors = plt.get_cmap("tab20")(i)

        # Because we fit all the points to RANSAC plane candidates
        # independently of the points density continuity, we have
        # artefacts depending on the order in which the planes are
        # detected.

        plane_model, inliers = rest.segment_plane(
            distance_threshold = 0.01,
            ransac_n = 3,
            num_iterations = 1000)

        tmp_inlier_pcd  = rest.select_by_index(inliers)

        # To prevent such behaviour we include a condition based on
        # Euclidean clustering to refine inlier point sets in conti-
        # guous clusters.

        labels = np.array(
            tmp_inlier_pcd.cluster_dbscan(
                eps = d_threshold * 10,         # empirical choice
                min_points = 10)
        )

        # Count how many points each cluster that we found holds,

        candidates = [
            len(np.where(labels == j)[0]) for j in np.unique(labels)
        ]

        # Now we have to find the 'best candidate', which is normally
        # the cluster that holds the more points!

        best_candidate = int(
            np.unique(labels)[
                np.where(
                    candidates == np.max(candidates)
                )[0]
            ]
        )

        print("the best candidate is: ", best_candidate)

        inlier_cloud  = tmp_inlier_pcd.select_by_index(
                            list(
                                np.where(
                                    labels == best_candidate
                                )[0]
                            )
                        )

        inlier_cloud.paint_uniform_color(list(colors[:3]))

        # The outlier_cloud, hold both the remaining points
        # from RANSAC and DBSCAN.

        outlier_cloud = (
            rest.select_by_index(inliers, invert = True) +
            tmp_inlier_pcd.select_by_index(
                list(
                    np.where(
                        labels != best_candidate
                    )[0]
                )
            )
        )

        segment_models[i] = plane_model
        segments[i]       = inlier_cloud
        rest              = outlier_cloud

        print(f"pass {i+1} / {max_plane_idx} done")

    o3d.visualization.draw_geometries([ rest ], **camera)

    #
    # Clustering of the remaining 3D points with DBSCAN
    #
    labels = np.array(
        rest.cluster_dbscan(
            eps = 0.05,
            min_points = 5)
    )

    max_label = labels.max()
    print(f"rest point cloud has {max_label + 1} clusters")

    colors = plt.get_cmap("tab10")(
        labels /
        (max_label if max_label > 0 else 1)
    )
    colors[labels < 0] = 0
    rest.colors = o3d.utility.Vector3dVector(colors[:, :3])

    o3d.visualization.draw_geometries([ rest ], **camera)

    o3d.visualization.draw_geometries(
        [ segments[i] for i in range(max_plane_idx) ]
       +[rest],
        ,**camera
    )
#+end_src

#+begin_src latex
\begin{figure}%
\centering
\subfloat[][Kitchen point cloud]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation6-1.PNG}
}
\qquad
\subfloat[][Remaining point cloud]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation6-2.PNG}
}
\qquad
\subfloat[][Final Segmentation]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation6-3.PNG}
}
\caption{Segmentation with RANSAC and DBSCAN.}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation with RANSAC and DBSCAN: Kitchen point cloud.
#+name:   fig:AutoSegmentation6-1
[[./AutoSegmentation6-1.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation with RANSAC and DBSCAN: Remaining points.
#+name:   fig:AutoSegmentation6-2
[[./AutoSegmentation6-2.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation with RANSAC and DBSCAN: Final Segmentation.
#+name:   fig:AutoSegmentation6-3
[[./AutoSegmentation6-3.PNG]]
#+end_export


#+begin_src latex
\begin{figure}%
\centering
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-1.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-2.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-3.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-4.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-5.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-6.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-7.PNG}
}
\qquad
\subfloat[][]{
  \includegraphics[width=.3\linewidth]{./AutoSegmentation7-8.PNG}
}
\caption{Segmentation of the Brotfabrik.e57 model.}
\label{fig:cont}%
\end{figure}
#+end_src

#+begin_export org
#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-1
[[./AutoSegmentation7-1.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-2
[[./AutoSegmentation7-2.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-3
[[./AutoSegmentation7-3.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-4
[[./AutoSegmentation7-4.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-5
[[./AutoSegmentation7-5.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-6
[[./AutoSegmentation7-6.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-7
[[./AutoSegmentation7-7.PNG]]

#+attr_html: :width 500px
#+attr_latex: :width 120px
#+caption: Segmentation of the Brotfabrik.e57 model.
#+name:   fig:AutoSegmentation7-8
[[./AutoSegmentation7-8.PNG]]
#+end_export

* Online Resources With Costs
** 3D Geodata Academy

The [[https://learngeodata.eu/][3D Geodata Academy]][[cite:&pouxGeodataAcademy]] provides online courses in the field of point clouds. It
is founded and operated by [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Florent Poux, Ph.D]] which is also also a Professor of the University
of Liège, Belgium[[cite:&pouxLiege]].

The Academy courses about point clouds are recommendable as a starting point for further investigations.
Poux focuses on hands-on teaching material and provides the daily experience that is hard to aquire.

References:
- [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Google Scholar Florent Poux, Ph.D]]
- [[https://www.researchgate.net/profile/Florent-Poux][ResearchGate Florent Poux]]
- [[https://www.uliege.be/cms/c_9054334/fr/repertoire?uid=u219618][Florent Poux, L'Université de Liège]]

*** Courses

| Course                | Prise |
|-----------------------+-------|
| [[https://learngeodata.eu/point-cloud-processor-formation/][Point Cloud Processor]] | 597€  |
| [[https://learngeodata.eu/3d-reconstructor-formation/][3D Reconstructor]]      | 397€  |

** Medium

[[https://medium.com/][Medium]] is a Online Publishing platform that allows writers to publish
content and earn some money in doing so. [[https://medium.com/@florentpoux][Florent Poux, Ph.D]][[cite:&pouxMedium]] is one of
these authors. He provides for instance the following articles:

- 3D Machine Learning 201 Guide: Point Cloud Semantic Segmentation
- 3D Point Cloud Clustering Tutorial with K-means and Python
- 3D Point Cloud Clustering Tutorial with K-means and Python
- How to automate 3D point cloud segmentation and clustering with Python
- Guide to real-time visualisation of massive 3D point clouds in Python
- How to automate LiDAR point cloud sub-sampling with Python
- Fundamentals to clustering high-dimensional data (3D point clouds)
- The Future of 3D Point Clouds: a new perspective

** FlyVast

[[https://flyvast.com/][FlyVast]][[cite:&pouxFlyFast]] is a webGL solution to process, manage, publish and share 3D scan data
(laser scanner, 3D sensor, LiDAR …) autonomously. This company is also founded
by [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Florent Poux, Ph.D]].

* Not Rated References                                              :publish:

Following some not rated references of work related to the field of interest:

 - Desktop 3D Scanning[[cite:&taubinDesktop3DScanning]]
 - Potree: Rendering Large Point Clouds in Web Browsers[[cite:&SCHUETZ-2016-POT]]
 - Real-Time Continuous Level of Detail Rendering of Point Clouds[[cite:&schutz_real-time_2019]]
 - Voxel Structure-Based Mesh Reconstruction From a 3D Point Cloud[[cite:&lv_voxel_2022]]
 - 3D Point Cloud Semantic Modelling: Integrated Framework for Indoor Spaces and Furniture[[cite:&poux_3d_2018]]
 - Efficient rendering of massive and dynamic point cloud data in state-of-the-art graphics engine[[cite:&media_efficient_nodate]]
 - Voxel-based 3D Point Cloud Semantic Segmentation: Unsupervised Geometric and Relationship Featuring vs Deep Learning Methods[[cite:&poux_voxel-based_2019]]
 - Self-Learning Ontology for Instance Segmentation of 3d Indoor Point Cloud[[cite:&poux_self-learning_2020]]
 - Compression and Rendering of Textured Point Clouds via Sparse Coding[[cite:&schuster_compression_2021]]
 - Real-Time Rendering of Point Clouds With Photorealistic Effects: A Survey[[cite:&kivi_real-time_2022]]

* Glossary
- BA :: Bundle Adjustment is a nonlinear optimization method to reduce the drift. It aimes
        at jointly refining a set of camera poses and 3D structure point estimates, by
        minimization of reprojection error.
- BIM :: Building Information Model.
- Dead Reckoning :: Intertial sensor navigation without GNSS.
- Drift :: Erros accumulation due to succesive image registrations.
- Feature :: Is a distinguishable local area of interest in the image.
             Therefore it can be uniquely described and matched across
             several images.

             This area, often a local neighborhood of point, corner or blob
             (area with shared property among pixels, such as color), can be
             described by a vector descriptor, which should be invariant to
             rotation, translation, scale and illumination changes in the
             ideal case.

             The term /feature/ comprises /feature extraction/ and /feature description/.

- Feature Description :: Creates a mathematical description (descriptors) of the
                         detected points of interest.

- Feature Detection :: Detects points of interest, i.e., points that are unique
                       and easily recognizable in the given image.

- Feature Extraction :: The process that takes an image as an input and returns
                        a set of features with descriptors. The process is composed of feature
                        detection and feature description.

- Feature Matching :: Is the comparison of descriptors across the given images.
                      As a result of this comparison, it identifies similar features.
                      The output of this stage is a set of tentative matches.
                      These are based solely on appearance and are not geometrically verified.

- Feature Selection Function :: The algorithm used by feature matching.
- Geometric Verification :: Filters incorrect matches by computing transformation
                            between two matched images. Matches corresponding to this transformation
                            are considered valid.
- GNSS :: Global Navigation Satellite System.
- GPS :: Global Positioning System
- ICP :: Iterative Closest Points
- IMU :: Inertial Measurement Unit
- LiDAR :: Light Detection and Ranging
- Manhattan-World assumptions :: Walls are assumed to be perfectly vertical and perpendicular to each other.
- MAV :: Micro Aerial Vehicles
- MVS :: Multi View Stereo is a successive step to SfM providing a dense 3D point cloud.
- Multi-Scale Problem :: Indoor space span four orders of magnitude in size.
- Perspective-n-Point (PnP) :: The problem is defined as using feature correspondences
                               to triangulate points in registered images.
                               PnP follows different routes according to the calibration of the camera.
                               Estimation of the pose of a calibrated camera is executed with a given set
                               of 3D points in the world and their equivalent 2D projections in the images.
                               RANSAC and minimal pose solver are used for this estimation.
                               For uncalibrated cameras, the intrinsic parameters are used with various
                               minimal solvers and sampling-based approaches.
- PVT :: Position, Velocity, precise Time.
- SfM :: Structure from Motion is a /photogrammetric range imaging/ technique for estimating three-dimensional
         structures from two-dimensional image sequences that may be coupled with local
         motion signals[[cite:&enwiki:1070706484]].

         It is an algorithm utilizing series of two-dimensional images and creating
         a three-dimensional structure of the object or scene captured in the images.
         The result of this process is a sparse 3D point cloud.

- SIFT :: Scale Invariant Feature Transform is a representative of hand-crafted local feature detection.

          I.e. it is a particular feature detector and descriptor algorithm, which is often used.
          SIFT algorithm works with gray-scale images.

- SLAM :: Simultaneous Localization And Mapping.
- Tentative Matches :: Matches are based solely on appearance, and it is not
                       guaranteed that all are correct correspondences of one 3D point
                       projected to two 2D images. Therefore geometrical verification
                       is necessary to find correct matches.
- TF-IDF :: Term Frequency-Inverse Document Frequency.
- TLS :: Terrestrial Laser Scanning.

* Bibliography

#+begin_export org
bibliographystyle:apacite
#+end_export
bibliography:../../../bibliography/bibliography.bib

* Project Management                                               :noexport:
- <2022-06-20 Mo> - <2022-06-24 Fr> :: First contact to the point cloud universe. Overview about point
                                       cloud file formats, vierwer software und libraries. Familiarizing
                                       with the /.e57/ file format and visualizing of the model
                                       /Brotfabrik.e57/. Conversion of this model into other common point
                                       cloud file formats.
- <2022-06-27 Mo> - <2022-07-01 Fr> :: Learning of Open3D handling and primitives.
- <2022-07-04 Mo> - <2022-07-08 Fr> :: Learning of Open3D handling and primitives.

* ToBe Discussed                                                   :noexport:
** 3D Geodata Academy

The [[https://learngeodata.eu/][3D Geodata Academy]][[cite:&pouxGeodataAcademy]] provides online courses in the field of point clouds. It
is founded by [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Florent Poux, Ph.D]] which is also a Professor of the University
of Liège, Belgium[[cite:&pouxLiege]].

References:
- [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Google Scholar Florent Poux, Ph.D]]
- [[https://www.researchgate.net/profile/Florent-Poux][ResearchGate Florent Poux]]
- [[https://www.uliege.be/cms/c_9054334/fr/repertoire?uid=u219618][Florent Poux, L'Université de Liège]]

*** Courses

| Course                | Prise |
|-----------------------+-------|
| [[https://learngeodata.eu/point-cloud-processor-formation/][Point Cloud Processor]] | 597€  |
| [[https://learngeodata.eu/3d-reconstructor-formation/][3D Reconstructor]]      | 397€  |

** Medium

[[https://medium.com/][Medium]] is a Online Publishing platform that allows writers to publish
content and earn some money in doing so. [[https://medium.com/@florentpoux][Florent Poux, Ph.D]][[cite:&pouxMedium]] is one of
these authors. He provides for instance the following articles:

- 3D Machine Learning 201 Guide: Point Cloud Semantic Segmentation
- 3D Point Cloud Clustering Tutorial with K-means and Python
- 3D Point Cloud Clustering Tutorial with K-means and Python
- How to automate 3D point cloud segmentation and clustering with Python
- Guide to real-time visualisation of massive 3D point clouds in Python
- How to automate LiDAR point cloud sub-sampling with Python
- Fundamentals to clustering high-dimensional data (3D point clouds)
- The Future of 3D Point Clouds: a new perspective

** FlyVast

[[https://flyvast.com/][FlyVast]][[cite:&pouxFlyFast]] is a webGL solution to process, manage, publish and share 3D scan data
(laser scanner, 3D sensor, LiDAR …) autonomously. This company is also founded
by [[https://scholar.google.com/citations?user=eoyJ6eYAAAAJ&hl=en][Florent Poux, Ph.D]].

** CUDA fähige Grafikkarte

A NVIDIA GPU that has CUDA capabilities is a necessary prerequisite
in point cloud evaluation and processing, especially with AI based
machine learning algorithms.


* Pipeline

The follwoing lists the pipeline procedure steps that must be applied
to get a final renderable /Visual Event/ artefact.

-  Renderable /Visual Event/ Artefact ::

   This is a low-polygonal mesh with hight quality textures that is easily
   renderable inside of /CADdy++/ and, even more important, in
   the /Visual Events Web-Viewer/. It is important that this artefact is
   transportable with minimal latency over a decent internet connection.

** Data aquisition

Three procedures are currently considered:

- Photogrammetry

  - Pro:
    - Best textures.
    - Cheap equipment.

  - Contra:
    - Worst on site data aquisition time.
    - Problematic on texture-less and glossy surfaces.
    - Problematic wrsp. to drift on large scale sites.
    - Most elaborate post-processing procedures.

  - Remark 2022/09/16:
    - We still do not have a completely understanding of the application space and the
      necessary tooling for this technique. It is recommeded to try to do some photogrammetry
      scans with a decent DSLR Camera like the /Sony Alpha R7 IV/ and a wide angle prime lense
      of good quality in order to get a final feeling of the results and quality that can
      be reached by applying this technique.

- LiDAR, e.g. NavVis VLX

  - Pro:
    - Best on site data aquisition time. Aquisition of the /DataSolid GmbH/ office space took
      only around 15 minutes real scanning time.
    - Perfect, homogeneous  point clouds without any noise.
    - Simple in-built large scale segmentation
    - Additional extra graph based /Point of Interest/ facility.

  - Contra:
    - Low quality textures that are probably not meeting our requirements
    - High equipment price.
    - Annual cloud computing costs.

- LiDAR + Photogrammetry

  The idea here is to take high quality textures with a DLSR and perform
  photogrammetry on these images in order to calculate the camera poses of
  the individual images. Then use these poses to perform texturing on
  the high quality lidar generated point cloud data.

  - Pro:
    - Best of both worlds

  - Contra:
    - Highest equipment price. You need both, the DSLR and a LiDAR scanner.
    - Technology at the time of writing not developed, but only theoretically
      understood.

** Noise reduction and decimation

Any data aquisition technique produces noise clutter. This noise must be reduced.

** Large Scale Point Cloud Segmentation

The aquired data must be segmented into managable point clouds that represent
point of interests like for instance an office room.

** Clutter removal

The aquired data might contain artefacts and clutter objects. These must be
removed before any further processing step.

** Point cloud decimation

The number of points in the point cloud must be adapted to a reasonable value.
Typically this means decimation of the number of aquired points. Voxelization
or point cloud homogenization must be performed.

** Segmentation of the point cloud

In this step the point cloud must be segmented into classes of point clouds.
For instance planar faces should be found and separated from the input
cloud. These segments should be separately processed in the subsequent stages.

** Decimation and simplification of the point clouds

The segmented point clouds can now be decimated and simplified depending
on their types. E.g. planar point clouds can be aggressively simplified.

** Meshing of the point clouds

The individual point clouds can now be meshed up.

** Repair and retopology of the point clouds

The point clouds need a final repair and retopology step.

** Texturing

The meshes can finally be textured.

** Re-Assembing of a final model.

* Tasks

The goal of any of the following tasks is to get an automated, semi-automated
or checklist working procedure that is as cheap as possible. The goal is
finally to get an economically reasonable  pipeline of procedure steps that
can be applied to the kind of point clouds we are dealing with.

** TODO Large Scale Point Cloud Segmentation                          :todo:

- Problem : The point clouds produced after photogrammetry or lidar scanning
            often stretch over multiple spaces of interest. E.g. rooms, lobbies,
            entrances or foyers. Subsequent point cloud processing steps require
            relative simple settings with a manageable amount of data points.
            Therefore it is desirable to segment the given point cloud to
            smaller entities.

- Constraints:

             - The coordinate frame is not allowed to change, i.e. no translation
               or rotation of the point cloud.
             - It is important that the separate segmented point clouds can
               be re-assembled later on.

- Goal: This segmentation step should be as cheap as possible. It would
        be desirable if it could be automated of semi-automated.
        At the minimum a checklist working procedure should be the result
        of this task.

** TODO Point Cloud Cleaning                                          :todo:

- Problem: We are facing two types of cluttering in any point cloud.

            - Imperfect data aquisition clutter due to noise.
            - Object points that we are not interested in for the task at hand.

** TODO Point Cloud Segmentation                                      :todo:

- Problem: We have a ready to use noise free point cloud, e.g. from LiDAR,
           and we would like to automatically segment planar surfaces for
           decimation processes. The point cloud is expected to span
           one room only, i.e. it is preprocessed by a coarse segmentation
           procedure.

           The segmented planar surfaces are replaced by eqivalent point
           clouds with low density for further meshing and texturing
           operations.

- Questions:

  - Is it possible to perform to perform this processing step with point
    clouds or is it necessary to do it on the meshed scene.

** TODO Meshing                                                       :todo:

- Problem: We need a robust automated meshing procedure of point clouds.
           It would be desirable if we coud perform meshing operations
           at the level of segmented clustered sub-point clouds, but
           without loosing the textureing capabilities.

** TODO Mesh Cleaning                                                 :todo:

- Problem: The resulting meshes always contain errors in the form
           of holes or bumpy surfaces that do need of repair.
           We need a simple, easy to learn robust workflow to perform
           such repair and retopology of meshes and still be able to
           retexture these meshes.

** TODO Textureing                                                    :todo:

** TODO Re-Assembling                                                 :todo:

* Libraries to evaluate                                            :noexport:
** laspy
* Youtube                                                          :noexport:
** Maziar Raissi

[[https://www.youtube.com/channel/UCxEiGqJ2e-Mg9oQMjVv6poQ][Maziar Raissi]] provides a great collection of lectures on deep learning
and point cloud processing. He has also a very informative [[https://github.com/maziarraissi][Github]] page.

* Geo Academy Courses                                              :noexport:
** 3D Reconstructor
** Point Cloud Processor
